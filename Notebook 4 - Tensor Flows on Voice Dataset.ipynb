{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>meanfreq</th>\n",
       "      <th>sd</th>\n",
       "      <th>median</th>\n",
       "      <th>Q25</th>\n",
       "      <th>Q75</th>\n",
       "      <th>IQR</th>\n",
       "      <th>skew</th>\n",
       "      <th>kurt</th>\n",
       "      <th>sp.ent</th>\n",
       "      <th>sfm</th>\n",
       "      <th>...</th>\n",
       "      <th>centroid</th>\n",
       "      <th>meanfun</th>\n",
       "      <th>minfun</th>\n",
       "      <th>maxfun</th>\n",
       "      <th>meandom</th>\n",
       "      <th>mindom</th>\n",
       "      <th>maxdom</th>\n",
       "      <th>dfrange</th>\n",
       "      <th>modindx</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.059781</td>\n",
       "      <td>0.064241</td>\n",
       "      <td>0.032027</td>\n",
       "      <td>0.015071</td>\n",
       "      <td>0.090193</td>\n",
       "      <td>0.075122</td>\n",
       "      <td>12.863462</td>\n",
       "      <td>274.402906</td>\n",
       "      <td>0.893369</td>\n",
       "      <td>0.491918</td>\n",
       "      <td>...</td>\n",
       "      <td>0.059781</td>\n",
       "      <td>0.084279</td>\n",
       "      <td>0.015702</td>\n",
       "      <td>0.275862</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.066009</td>\n",
       "      <td>0.067310</td>\n",
       "      <td>0.040229</td>\n",
       "      <td>0.019414</td>\n",
       "      <td>0.092666</td>\n",
       "      <td>0.073252</td>\n",
       "      <td>22.423285</td>\n",
       "      <td>634.613855</td>\n",
       "      <td>0.892193</td>\n",
       "      <td>0.513724</td>\n",
       "      <td>...</td>\n",
       "      <td>0.066009</td>\n",
       "      <td>0.107937</td>\n",
       "      <td>0.015826</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.009014</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.054688</td>\n",
       "      <td>0.046875</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.077316</td>\n",
       "      <td>0.083829</td>\n",
       "      <td>0.036718</td>\n",
       "      <td>0.008701</td>\n",
       "      <td>0.131908</td>\n",
       "      <td>0.123207</td>\n",
       "      <td>30.757155</td>\n",
       "      <td>1024.927705</td>\n",
       "      <td>0.846389</td>\n",
       "      <td>0.478905</td>\n",
       "      <td>...</td>\n",
       "      <td>0.077316</td>\n",
       "      <td>0.098706</td>\n",
       "      <td>0.015656</td>\n",
       "      <td>0.271186</td>\n",
       "      <td>0.007990</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.046512</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.151228</td>\n",
       "      <td>0.072111</td>\n",
       "      <td>0.158011</td>\n",
       "      <td>0.096582</td>\n",
       "      <td>0.207955</td>\n",
       "      <td>0.111374</td>\n",
       "      <td>1.232831</td>\n",
       "      <td>4.177296</td>\n",
       "      <td>0.963322</td>\n",
       "      <td>0.727232</td>\n",
       "      <td>...</td>\n",
       "      <td>0.151228</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.017798</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.201497</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.554688</td>\n",
       "      <td>0.247119</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.135120</td>\n",
       "      <td>0.079146</td>\n",
       "      <td>0.124656</td>\n",
       "      <td>0.078720</td>\n",
       "      <td>0.206045</td>\n",
       "      <td>0.127325</td>\n",
       "      <td>1.101174</td>\n",
       "      <td>4.333713</td>\n",
       "      <td>0.971955</td>\n",
       "      <td>0.783568</td>\n",
       "      <td>...</td>\n",
       "      <td>0.135120</td>\n",
       "      <td>0.106398</td>\n",
       "      <td>0.016931</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.712812</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>5.484375</td>\n",
       "      <td>5.476562</td>\n",
       "      <td>0.208274</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   meanfreq        sd    median       Q25       Q75       IQR       skew  \\\n",
       "0  0.059781  0.064241  0.032027  0.015071  0.090193  0.075122  12.863462   \n",
       "1  0.066009  0.067310  0.040229  0.019414  0.092666  0.073252  22.423285   \n",
       "2  0.077316  0.083829  0.036718  0.008701  0.131908  0.123207  30.757155   \n",
       "3  0.151228  0.072111  0.158011  0.096582  0.207955  0.111374   1.232831   \n",
       "4  0.135120  0.079146  0.124656  0.078720  0.206045  0.127325   1.101174   \n",
       "\n",
       "          kurt    sp.ent       sfm  ...    centroid   meanfun    minfun  \\\n",
       "0   274.402906  0.893369  0.491918  ...    0.059781  0.084279  0.015702   \n",
       "1   634.613855  0.892193  0.513724  ...    0.066009  0.107937  0.015826   \n",
       "2  1024.927705  0.846389  0.478905  ...    0.077316  0.098706  0.015656   \n",
       "3     4.177296  0.963322  0.727232  ...    0.151228  0.088965  0.017798   \n",
       "4     4.333713  0.971955  0.783568  ...    0.135120  0.106398  0.016931   \n",
       "\n",
       "     maxfun   meandom    mindom    maxdom   dfrange   modindx  label  \n",
       "0  0.275862  0.007812  0.007812  0.007812  0.000000  0.000000   male  \n",
       "1  0.250000  0.009014  0.007812  0.054688  0.046875  0.052632   male  \n",
       "2  0.271186  0.007990  0.007812  0.015625  0.007812  0.046512   male  \n",
       "3  0.250000  0.201497  0.007812  0.562500  0.554688  0.247119   male  \n",
       "4  0.266667  0.712812  0.007812  5.484375  5.476562  0.208274   male  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import pandas as pds\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "dataframeX = pds.read_csv('/Users/neelambabel/ADS Folder/Assignment 2/voice-data.csv')\n",
    "\n",
    "dataframeX.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Case 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   meanfreq        sd    median       Q25       Q75       IQR       skew  \\\n",
      "0  0.059781  0.064241  0.032027  0.015071  0.090193  0.075122  12.863462   \n",
      "1  0.066009  0.067310  0.040229  0.019414  0.092666  0.073252  22.423285   \n",
      "2  0.077316  0.083829  0.036718  0.008701  0.131908  0.123207  30.757155   \n",
      "3  0.151228  0.072111  0.158011  0.096582  0.207955  0.111374   1.232831   \n",
      "4  0.135120  0.079146  0.124656  0.078720  0.206045  0.127325   1.101174   \n",
      "\n",
      "          kurt    sp.ent       sfm      mode  centroid   meanfun    minfun  \\\n",
      "0   274.402906  0.893369  0.491918  0.000000  0.059781  0.084279  0.015702   \n",
      "1   634.613855  0.892193  0.513724  0.000000  0.066009  0.107937  0.015826   \n",
      "2  1024.927705  0.846389  0.478905  0.000000  0.077316  0.098706  0.015656   \n",
      "3     4.177296  0.963322  0.727232  0.083878  0.151228  0.088965  0.017798   \n",
      "4     4.333713  0.971955  0.783568  0.104261  0.135120  0.106398  0.016931   \n",
      "\n",
      "     maxfun   meandom    mindom    maxdom   dfrange   modindx  \n",
      "0  0.275862  0.007812  0.007812  0.007812  0.000000  0.000000  \n",
      "1  0.250000  0.009014  0.007812  0.054688  0.046875  0.052632  \n",
      "2  0.271186  0.007990  0.007812  0.015625  0.007812  0.046512  \n",
      "3  0.250000  0.201497  0.007812  0.562500  0.554688  0.247119  \n",
      "4  0.266667  0.712812  0.007812  5.484375  5.476562  0.208274  \n"
     ]
    }
   ],
   "source": [
    "dataframeX = pds.read_csv('/Users/neelambabel/ADS Folder/Assignment 2/voice-data.csv', usecols=[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19])\n",
    "print(dataframeX.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  label\n",
      "0  male\n",
      "1  male\n",
      "2  male\n",
      "3  male\n",
      "4  male\n"
     ]
    }
   ],
   "source": [
    "dataframeY = pds.read_csv('/Users/neelambabel/ADS Folder/Assignment 2/voice-data.csv', usecols=[20])\n",
    "print(dataframeY.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   label\n",
      "0      0\n",
      "1      0\n",
      "2      0\n",
      "3      0\n",
      "4      0\n",
      "      label\n",
      "3163      1\n",
      "3164      1\n",
      "3165      1\n",
      "3166      1\n",
      "3167      1\n"
     ]
    }
   ],
   "source": [
    "def labelToInt(label):\n",
    "    if label == 'male':\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "    \n",
    "dataframeY.label = dataframeY.label.apply(labelToInt)\n",
    "print(dataframeY.head())\n",
    "print(dataframeY.tail())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3168, 20)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframeX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3168, 1)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframeY.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_29 (Dense)             (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 10)                210       \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/neelambabel/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:11: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(20, input_shape=(20,), activation=\"sigmoid\", kernel_initializer=\"uniform\")`\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/Users/neelambabel/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:12: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(10, activation=\"sigmoid\", kernel_initializer=\"uniform\")`\n",
      "  if sys.path[0] == '':\n",
      "/Users/neelambabel/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:13: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(10, activation=\"sigmoid\", kernel_initializer=\"uniform\")`\n",
      "  del sys.path[0]\n",
      "/Users/neelambabel/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:14: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1, activation=\"sigmoid\", kernel_initializer=\"uniform\")`\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total params: 751\n",
      "Trainable params: 751\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 2534 samples, validate on 634 samples\n",
      "Epoch 1/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.2410 - acc: 0.6251 - val_loss: 0.3301 - val_acc: 0.0000e+00\n",
      "Epoch 2/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.2354 - acc: 0.6251 - val_loss: 0.3763 - val_acc: 0.0000e+00\n",
      "Epoch 3/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.2343 - acc: 0.6251 - val_loss: 0.3915 - val_acc: 0.0000e+00\n",
      "Epoch 4/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.2343 - acc: 0.6251 - val_loss: 0.3887 - val_acc: 0.0000e+00\n",
      "Epoch 5/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.2341 - acc: 0.6251 - val_loss: 0.3851 - val_acc: 0.0000e+00\n",
      "Epoch 6/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.2339 - acc: 0.6251 - val_loss: 0.3957 - val_acc: 0.0000e+00\n",
      "Epoch 7/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.2337 - acc: 0.6251 - val_loss: 0.3924 - val_acc: 0.0000e+00\n",
      "Epoch 8/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.2335 - acc: 0.6251 - val_loss: 0.3943 - val_acc: 0.0000e+00\n",
      "Epoch 9/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.2332 - acc: 0.6251 - val_loss: 0.3930 - val_acc: 0.0000e+00\n",
      "Epoch 10/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.2330 - acc: 0.6251 - val_loss: 0.3916 - val_acc: 0.0000e+00\n",
      "Epoch 11/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.2326 - acc: 0.6251 - val_loss: 0.3936 - val_acc: 0.0000e+00\n",
      "Epoch 12/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.2322 - acc: 0.6251 - val_loss: 0.3982 - val_acc: 0.0000e+00\n",
      "Epoch 13/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.2320 - acc: 0.6251 - val_loss: 0.4014 - val_acc: 0.0000e+00\n",
      "Epoch 14/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.2313 - acc: 0.6251 - val_loss: 0.3974 - val_acc: 0.0000e+00\n",
      "Epoch 15/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.2306 - acc: 0.6251 - val_loss: 0.3999 - val_acc: 0.0000e+00\n",
      "Epoch 16/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.2298 - acc: 0.6251 - val_loss: 0.3874 - val_acc: 0.0000e+00\n",
      "Epoch 17/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.2287 - acc: 0.6251 - val_loss: 0.3922 - val_acc: 0.0000e+00\n",
      "Epoch 18/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.2277 - acc: 0.6251 - val_loss: 0.3936 - val_acc: 0.0000e+00\n",
      "Epoch 19/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.2261 - acc: 0.6251 - val_loss: 0.3939 - val_acc: 0.0000e+00\n",
      "Epoch 20/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.2243 - acc: 0.6251 - val_loss: 0.3843 - val_acc: 0.0000e+00\n",
      "Epoch 21/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.2222 - acc: 0.6251 - val_loss: 0.3830 - val_acc: 0.0000e+00\n",
      "Epoch 22/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.2200 - acc: 0.6251 - val_loss: 0.3762 - val_acc: 0.0000e+00\n",
      "Epoch 23/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.2169 - acc: 0.6251 - val_loss: 0.3743 - val_acc: 0.0000e+00\n",
      "Epoch 24/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.2138 - acc: 0.6251 - val_loss: 0.3636 - val_acc: 0.0000e+00\n",
      "Epoch 25/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.2105 - acc: 0.6251 - val_loss: 0.3508 - val_acc: 0.0000e+00\n",
      "Epoch 26/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.2065 - acc: 0.6251 - val_loss: 0.3533 - val_acc: 0.0000e+00\n",
      "Epoch 27/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.2026 - acc: 0.6405 - val_loss: 0.3506 - val_acc: 0.1404\n",
      "Epoch 28/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.1985 - acc: 0.6768 - val_loss: 0.3184 - val_acc: 0.3707\n",
      "Epoch 29/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.1943 - acc: 0.7447 - val_loss: 0.3204 - val_acc: 0.4117\n",
      "Epoch 30/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.1899 - acc: 0.7620 - val_loss: 0.3111 - val_acc: 0.49050.76\n",
      "Epoch 31/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.1857 - acc: 0.7778 - val_loss: 0.3102 - val_acc: 0.5110\n",
      "Epoch 32/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.1815 - acc: 0.7889 - val_loss: 0.2775 - val_acc: 0.6388\n",
      "Epoch 33/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.1775 - acc: 0.7960 - val_loss: 0.2787 - val_acc: 0.6309\n",
      "Epoch 34/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.1741 - acc: 0.8027 - val_loss: 0.2753 - val_acc: 0.6341\n",
      "Epoch 35/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.1699 - acc: 0.8090 - val_loss: 0.2431 - val_acc: 0.7066\n",
      "Epoch 36/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.1667 - acc: 0.8078 - val_loss: 0.2542 - val_acc: 0.6814\n",
      "Epoch 37/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.1630 - acc: 0.8129 - val_loss: 0.2538 - val_acc: 0.6814\n",
      "Epoch 38/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.1599 - acc: 0.8153 - val_loss: 0.2423 - val_acc: 0.7050\n",
      "Epoch 39/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.1569 - acc: 0.8200 - val_loss: 0.2239 - val_acc: 0.7350\n",
      "Epoch 40/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.1534 - acc: 0.8197 - val_loss: 0.2374 - val_acc: 0.6956\n",
      "Epoch 41/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.1506 - acc: 0.8260 - val_loss: 0.2314 - val_acc: 0.7114\n",
      "Epoch 42/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.1479 - acc: 0.8299 - val_loss: 0.2181 - val_acc: 0.7287\n",
      "Epoch 43/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.1452 - acc: 0.8331 - val_loss: 0.2284 - val_acc: 0.7082\n",
      "Epoch 44/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.1427 - acc: 0.8350 - val_loss: 0.2134 - val_acc: 0.7366\n",
      "Epoch 45/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.1410 - acc: 0.8350 - val_loss: 0.2048 - val_acc: 0.7413\n",
      "Epoch 46/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.1376 - acc: 0.8437 - val_loss: 0.2108 - val_acc: 0.7366\n",
      "Epoch 47/100\n",
      "2534/2534 [==============================] - ETA: 0s - loss: 0.1359 - acc: 0.845 - 0s - loss: 0.1359 - acc: 0.8441 - val_loss: 0.2133 - val_acc: 0.7319\n",
      "Epoch 48/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.1341 - acc: 0.8457 - val_loss: 0.1955 - val_acc: 0.7571\n",
      "Epoch 49/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.1331 - acc: 0.8433 - val_loss: 0.1984 - val_acc: 0.7524\n",
      "Epoch 50/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.1304 - acc: 0.8500 - val_loss: 0.2126 - val_acc: 0.7303\n",
      "Epoch 51/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.1284 - acc: 0.8508 - val_loss: 0.1925 - val_acc: 0.7539\n",
      "Epoch 52/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.1272 - acc: 0.8536 - val_loss: 0.1877 - val_acc: 0.7603\n",
      "Epoch 53/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.1253 - acc: 0.8520 - val_loss: 0.1589 - val_acc: 0.7934\n",
      "Epoch 54/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.1249 - acc: 0.8540 - val_loss: 0.1791 - val_acc: 0.7681\n",
      "Epoch 55/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.1220 - acc: 0.8587 - val_loss: 0.1870 - val_acc: 0.7603\n",
      "Epoch 56/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.1213 - acc: 0.8591 - val_loss: 0.1873 - val_acc: 0.7571\n",
      "Epoch 57/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.1198 - acc: 0.8603 - val_loss: 0.1930 - val_acc: 0.7508\n",
      "Epoch 58/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.1184 - acc: 0.8607 - val_loss: 0.1698 - val_acc: 0.7713\n",
      "Epoch 59/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.1164 - acc: 0.8654 - val_loss: 0.1691 - val_acc: 0.7729\n",
      "Epoch 60/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.1158 - acc: 0.8666 - val_loss: 0.1854 - val_acc: 0.7571\n",
      "Epoch 61/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.1140 - acc: 0.8686 - val_loss: 0.1788 - val_acc: 0.7666\n",
      "Epoch 62/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.1129 - acc: 0.8670 - val_loss: 0.1752 - val_acc: 0.7697\n",
      "Epoch 63/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2534/2534 [==============================] - 0s - loss: 0.1116 - acc: 0.8694 - val_loss: 0.1888 - val_acc: 0.7508\n",
      "Epoch 64/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.1113 - acc: 0.8725 - val_loss: 0.1629 - val_acc: 0.7823\n",
      "Epoch 65/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.1094 - acc: 0.8717 - val_loss: 0.1649 - val_acc: 0.7792\n",
      "Epoch 66/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.1085 - acc: 0.8717 - val_loss: 0.1577 - val_acc: 0.7871\n",
      "Epoch 67/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.1073 - acc: 0.8773 - val_loss: 0.1512 - val_acc: 0.7981\n",
      "Epoch 68/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.1070 - acc: 0.8781 - val_loss: 0.1438 - val_acc: 0.8044\n",
      "Epoch 69/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.1051 - acc: 0.8781 - val_loss: 0.1569 - val_acc: 0.7855\n",
      "Epoch 70/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.1043 - acc: 0.8820 - val_loss: 0.1339 - val_acc: 0.8233\n",
      "Epoch 71/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.1032 - acc: 0.8832 - val_loss: 0.1632 - val_acc: 0.7792\n",
      "Epoch 72/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.1017 - acc: 0.8879 - val_loss: 0.1364 - val_acc: 0.8139\n",
      "Epoch 73/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.1018 - acc: 0.8816 - val_loss: 0.1481 - val_acc: 0.7934\n",
      "Epoch 74/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.1007 - acc: 0.8824 - val_loss: 0.1416 - val_acc: 0.8107\n",
      "Epoch 75/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.0992 - acc: 0.8879 - val_loss: 0.1567 - val_acc: 0.7855\n",
      "Epoch 76/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.0995 - acc: 0.8848 - val_loss: 0.1453 - val_acc: 0.8013\n",
      "Epoch 77/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.0975 - acc: 0.8879 - val_loss: 0.1317 - val_acc: 0.8218\n",
      "Epoch 78/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.0967 - acc: 0.8919 - val_loss: 0.1363 - val_acc: 0.8107\n",
      "Epoch 79/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.0968 - acc: 0.8907 - val_loss: 0.1431 - val_acc: 0.8028\n",
      "Epoch 80/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.0952 - acc: 0.8923 - val_loss: 0.1594 - val_acc: 0.7823\n",
      "Epoch 81/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.0954 - acc: 0.8919 - val_loss: 0.1367 - val_acc: 0.8123\n",
      "Epoch 82/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.0940 - acc: 0.8938 - val_loss: 0.1268 - val_acc: 0.8297\n",
      "Epoch 83/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.0928 - acc: 0.8950 - val_loss: 0.1361 - val_acc: 0.8107\n",
      "Epoch 84/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.0929 - acc: 0.8970 - val_loss: 0.1369 - val_acc: 0.8107\n",
      "Epoch 85/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.0935 - acc: 0.8923 - val_loss: 0.1312 - val_acc: 0.8249\n",
      "Epoch 86/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.0925 - acc: 0.8942 - val_loss: 0.1324 - val_acc: 0.8186\n",
      "Epoch 87/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.0912 - acc: 0.8958 - val_loss: 0.1185 - val_acc: 0.8438\n",
      "Epoch 88/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.0896 - acc: 0.8998 - val_loss: 0.1269 - val_acc: 0.8265\n",
      "Epoch 89/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.0905 - acc: 0.8978 - val_loss: 0.1553 - val_acc: 0.7871\n",
      "Epoch 90/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.0888 - acc: 0.9009 - val_loss: 0.1251 - val_acc: 0.8312\n",
      "Epoch 91/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.0883 - acc: 0.9009 - val_loss: 0.1098 - val_acc: 0.8517\n",
      "Epoch 92/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.0874 - acc: 0.9033 - val_loss: 0.1309 - val_acc: 0.8202\n",
      "Epoch 93/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.0870 - acc: 0.9033 - val_loss: 0.1103 - val_acc: 0.8549\n",
      "Epoch 94/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.0865 - acc: 0.9033 - val_loss: 0.1240 - val_acc: 0.8344\n",
      "Epoch 95/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.0863 - acc: 0.9033 - val_loss: 0.1322 - val_acc: 0.8170\n",
      "Epoch 96/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.0858 - acc: 0.9029 - val_loss: 0.1145 - val_acc: 0.8502\n",
      "Epoch 97/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.0854 - acc: 0.9029 - val_loss: 0.1261 - val_acc: 0.8297\n",
      "Epoch 98/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.0847 - acc: 0.9053 - val_loss: 0.1314 - val_acc: 0.8249\n",
      "Epoch 99/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.0842 - acc: 0.9065 - val_loss: 0.1288 - val_acc: 0.8297\n",
      "Epoch 100/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.0835 - acc: 0.9065 - val_loss: 0.1289 - val_acc: 0.8249\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1206a1e10>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "# 1\n",
    "random.seed(datetime.now())\n",
    "\n",
    "# 2\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "model = Sequential()\n",
    "model.add(Dense(20, input_shape=(20,), init='uniform', activation='sigmoid'))\n",
    "model.add(Dense(10,init='uniform', activation='sigmoid'))\n",
    "model.add(Dense(10, init='uniform', activation='sigmoid'))\n",
    "model.add(Dense(1, init='uniform', activation='sigmoid'))\n",
    "model.summary()\n",
    "\n",
    "# 3\n",
    "import keras\n",
    "tbCallBack = keras.callbacks.TensorBoard(log_dir='/tmp/keras_logs', write_graph=True)\n",
    "\n",
    "# 4\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(dataframeX.values, dataframeY.values, epochs=100, batch_size=50,  verbose=1, validation_split=0.2, callbacks=[tbCallBack])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy ~85%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Case 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reducing the Input Columns, increasing the number of Neurons and increasing the Epoch Value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   meanfreq        sd    median       IQR    sp.ent       sfm      mode  \\\n",
      "0  0.059781  0.064241  0.032027  0.075122  0.893369  0.491918  0.000000   \n",
      "1  0.066009  0.067310  0.040229  0.073252  0.892193  0.513724  0.000000   \n",
      "2  0.077316  0.083829  0.036718  0.123207  0.846389  0.478905  0.000000   \n",
      "3  0.151228  0.072111  0.158011  0.111374  0.963322  0.727232  0.083878   \n",
      "4  0.135120  0.079146  0.124656  0.127325  0.971955  0.783568  0.104261   \n",
      "\n",
      "   centroid   meanfun   dfrange   modindx  \n",
      "0  0.059781  0.084279  0.000000  0.000000  \n",
      "1  0.066009  0.107937  0.046875  0.052632  \n",
      "2  0.077316  0.098706  0.007812  0.046512  \n",
      "3  0.151228  0.088965  0.554688  0.247119  \n",
      "4  0.135120  0.106398  5.476562  0.208274  \n"
     ]
    }
   ],
   "source": [
    "dataframeX = pds.read_csv('/Users/neelambabel/ADS Folder/Assignment 2/voice-data.csv', usecols=[0,1,2,5,8,9,10,11,12,18,19])\n",
    "print(dataframeX.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  label\n",
      "0  male\n",
      "1  male\n",
      "2  male\n",
      "3  male\n",
      "4  male\n"
     ]
    }
   ],
   "source": [
    "dataframeY = pds.read_csv('/Users/neelambabel/ADS Folder/Assignment 2/voice-data.csv', usecols=[20])\n",
    "print(dataframeY.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   label\n",
      "0      0\n",
      "1      0\n",
      "2      0\n",
      "3      0\n",
      "4      0\n",
      "      label\n",
      "3163      1\n",
      "3164      1\n",
      "3165      1\n",
      "3166      1\n",
      "3167      1\n"
     ]
    }
   ],
   "source": [
    "def labelToInt(label):\n",
    "    if label == 'male':\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "    \n",
    "dataframeY.label = dataframeY.label.apply(labelToInt)\n",
    "print(dataframeY.head())\n",
    "print(dataframeY.tail())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3168, 11)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframeX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3168, 1)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframeY.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/neelambabel/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:11: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(11, input_shape=(11,), activation=\"sigmoid\", kernel_initializer=\"uniform\")`\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/Users/neelambabel/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:12: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(11, activation=\"sigmoid\", kernel_initializer=\"uniform\")`\n",
      "  if sys.path[0] == '':\n",
      "/Users/neelambabel/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:13: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(11, activation=\"sigmoid\", kernel_initializer=\"uniform\")`\n",
      "  del sys.path[0]\n",
      "/Users/neelambabel/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:14: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1, activation=\"sigmoid\", kernel_initializer=\"uniform\")`\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_41 (Dense)             (None, 11)                132       \n",
      "_________________________________________________________________\n",
      "dense_42 (Dense)             (None, 11)                132       \n",
      "_________________________________________________________________\n",
      "dense_43 (Dense)             (None, 11)                132       \n",
      "_________________________________________________________________\n",
      "dense_44 (Dense)             (None, 1)                 12        \n",
      "=================================================================\n",
      "Total params: 408\n",
      "Trainable params: 408\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 2534 samples, validate on 634 samples\n",
      "Epoch 1/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.2408 - acc: 0.6251 - val_loss: 0.3389 - val_acc: 0.0000e+00\n",
      "Epoch 2/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.2350 - acc: 0.6251 - val_loss: 0.3819 - val_acc: 0.0000e+00\n",
      "Epoch 3/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.2344 - acc: 0.6251 - val_loss: 0.3914 - val_acc: 0.0000e+00\n",
      "Epoch 4/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.2343 - acc: 0.6251 - val_loss: 0.3926 - val_acc: 0.0000e+00\n",
      "Epoch 5/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.2343 - acc: 0.6251 - val_loss: 0.3926 - val_acc: 0.0000e+00\n",
      "Epoch 6/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.2342 - acc: 0.6251 - val_loss: 0.3884 - val_acc: 0.0000e+00\n",
      "Epoch 7/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.2342 - acc: 0.6251 - val_loss: 0.3941 - val_acc: 0.0000e+00\n",
      "Epoch 8/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.2342 - acc: 0.6251 - val_loss: 0.3921 - val_acc: 0.0000e+00\n",
      "Epoch 9/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.2341 - acc: 0.6251 - val_loss: 0.3882 - val_acc: 0.0000e+00\n",
      "Epoch 10/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.2341 - acc: 0.6251 - val_loss: 0.3898 - val_acc: 0.0000e+00\n",
      "Epoch 11/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.2339 - acc: 0.6251 - val_loss: 0.3915 - val_acc: 0.0000e+00\n",
      "Epoch 12/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.2339 - acc: 0.6251 - val_loss: 0.3863 - val_acc: 0.0000e+00\n",
      "Epoch 13/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.2338 - acc: 0.6251 - val_loss: 0.3931 - val_acc: 0.0000e+00\n",
      "Epoch 14/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.2339 - acc: 0.6251 - val_loss: 0.3904 - val_acc: 0.0000e+00\n",
      "Epoch 15/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.2336 - acc: 0.6251 - val_loss: 0.3866 - val_acc: 0.0000e+00\n",
      "Epoch 16/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.2335 - acc: 0.6251 - val_loss: 0.3916 - val_acc: 0.0000e+00\n",
      "Epoch 17/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.2333 - acc: 0.6251 - val_loss: 0.4003 - val_acc: 0.0000e+00\n",
      "Epoch 18/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.2332 - acc: 0.6251 - val_loss: 0.3884 - val_acc: 0.0000e+00\n",
      "Epoch 19/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.2331 - acc: 0.6251 - val_loss: 0.3971 - val_acc: 0.0000e+00\n",
      "Epoch 20/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.2330 - acc: 0.6251 - val_loss: 0.3866 - val_acc: 0.0000e+00\n",
      "Epoch 21/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.2327 - acc: 0.6251 - val_loss: 0.3961 - val_acc: 0.0000e+00\n",
      "Epoch 22/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.2327 - acc: 0.6251 - val_loss: 0.3895 - val_acc: 0.0000e+00\n",
      "Epoch 23/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.2326 - acc: 0.6251 - val_loss: 0.3917 - val_acc: 0.0000e+00\n",
      "Epoch 24/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.2322 - acc: 0.6251 - val_loss: 0.4000 - val_acc: 0.0000e+00\n",
      "Epoch 25/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.2321 - acc: 0.6251 - val_loss: 0.3851 - val_acc: 0.0000e+00\n",
      "Epoch 26/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.2316 - acc: 0.6251 - val_loss: 0.3911 - val_acc: 0.0000e+00\n",
      "Epoch 27/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.2314 - acc: 0.6251 - val_loss: 0.3918 - val_acc: 0.0000e+00\n",
      "Epoch 28/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.2309 - acc: 0.6251 - val_loss: 0.3834 - val_acc: 0.0000e+00\n",
      "Epoch 29/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.2305 - acc: 0.6251 - val_loss: 0.3877 - val_acc: 0.0000e+00\n",
      "Epoch 30/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.2300 - acc: 0.6251 - val_loss: 0.3899 - val_acc: 0.0000e+00\n",
      "Epoch 31/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.2294 - acc: 0.6251 - val_loss: 0.3867 - val_acc: 0.0000e+00\n",
      "Epoch 32/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.2289 - acc: 0.6251 - val_loss: 0.3788 - val_acc: 0.0000e+00\n",
      "Epoch 33/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.2282 - acc: 0.6251 - val_loss: 0.3879 - val_acc: 0.0000e+00\n",
      "Epoch 34/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.2271 - acc: 0.6251 - val_loss: 0.3786 - val_acc: 0.0000e+00\n",
      "Epoch 35/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.2261 - acc: 0.6251 - val_loss: 0.3782 - val_acc: 0.0000e+00\n",
      "Epoch 36/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.2250 - acc: 0.6251 - val_loss: 0.3705 - val_acc: 0.0000e+00\n",
      "Epoch 37/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.2238 - acc: 0.6251 - val_loss: 0.3706 - val_acc: 0.0000e+00\n",
      "Epoch 38/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.2225 - acc: 0.6251 - val_loss: 0.3676 - val_acc: 0.0000e+00\n",
      "Epoch 39/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.2212 - acc: 0.6251 - val_loss: 0.3613 - val_acc: 0.0000e+00\n",
      "Epoch 40/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.2197 - acc: 0.6251 - val_loss: 0.3630 - val_acc: 0.0000e+00\n",
      "Epoch 41/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.2182 - acc: 0.6267 - val_loss: 0.3516 - val_acc: 0.0363\n",
      "Epoch 42/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.2169 - acc: 0.6346 - val_loss: 0.3418 - val_acc: 0.3218\n",
      "Epoch 43/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.2154 - acc: 0.6764 - val_loss: 0.3371 - val_acc: 0.4196\n",
      "Epoch 44/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.2137 - acc: 0.6764 - val_loss: 0.3427 - val_acc: 0.4259\n",
      "Epoch 45/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.2124 - acc: 0.6744 - val_loss: 0.3385 - val_acc: 0.4637\n",
      "Epoch 46/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.2110 - acc: 0.6784 - val_loss: 0.3315 - val_acc: 0.4826\n",
      "Epoch 47/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.2097 - acc: 0.6811 - val_loss: 0.3258 - val_acc: 0.4937\n",
      "Epoch 48/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.2083 - acc: 0.6906 - val_loss: 0.3254 - val_acc: 0.4937\n",
      "Epoch 49/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.2071 - acc: 0.6993 - val_loss: 0.3165 - val_acc: 0.5110\n",
      "Epoch 50/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.2059 - acc: 0.7005 - val_loss: 0.3118 - val_acc: 0.5205\n",
      "Epoch 51/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.2050 - acc: 0.7060 - val_loss: 0.3129 - val_acc: 0.5221\n",
      "Epoch 52/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.2036 - acc: 0.7088 - val_loss: 0.3050 - val_acc: 0.5284\n",
      "Epoch 53/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.2027 - acc: 0.7103 - val_loss: 0.3060 - val_acc: 0.5237\n",
      "Epoch 54/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.2018 - acc: 0.7155 - val_loss: 0.2940 - val_acc: 0.5363\n",
      "Epoch 55/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.2006 - acc: 0.7186 - val_loss: 0.3017 - val_acc: 0.5268\n",
      "Epoch 56/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.1996 - acc: 0.7214 - val_loss: 0.2947 - val_acc: 0.5331\n",
      "Epoch 57/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.1986 - acc: 0.7261 - val_loss: 0.2935 - val_acc: 0.5331\n",
      "Epoch 58/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.1975 - acc: 0.7273 - val_loss: 0.2871 - val_acc: 0.5363\n",
      "Epoch 59/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.1965 - acc: 0.7309 - val_loss: 0.2920 - val_acc: 0.5331\n",
      "Epoch 60/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.1954 - acc: 0.7344 - val_loss: 0.2890 - val_acc: 0.5331\n",
      "Epoch 61/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.1944 - acc: 0.7348 - val_loss: 0.2883 - val_acc: 0.5363\n",
      "Epoch 62/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.1932 - acc: 0.7348 - val_loss: 0.2816 - val_acc: 0.5457\n",
      "Epoch 63/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.1918 - acc: 0.7403 - val_loss: 0.2820 - val_acc: 0.5426\n",
      "Epoch 64/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.1906 - acc: 0.7447 - val_loss: 0.2753 - val_acc: 0.5521\n",
      "Epoch 65/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.1895 - acc: 0.7463 - val_loss: 0.2835 - val_acc: 0.5394\n",
      "Epoch 66/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.1878 - acc: 0.7514 - val_loss: 0.2649 - val_acc: 0.5694\n",
      "Epoch 67/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.1867 - acc: 0.7498 - val_loss: 0.2665 - val_acc: 0.5584\n",
      "Epoch 68/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.1851 - acc: 0.7537 - val_loss: 0.2591 - val_acc: 0.5789\n",
      "Epoch 69/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.1837 - acc: 0.7565 - val_loss: 0.2649 - val_acc: 0.5678\n",
      "Epoch 70/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.1819 - acc: 0.7601 - val_loss: 0.2635 - val_acc: 0.5757\n",
      "Epoch 71/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.1803 - acc: 0.7620 - val_loss: 0.2673 - val_acc: 0.5694\n",
      "Epoch 72/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.1785 - acc: 0.7668 - val_loss: 0.2576 - val_acc: 0.5820\n",
      "Epoch 73/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.1765 - acc: 0.7755 - val_loss: 0.2440 - val_acc: 0.5962\n",
      "Epoch 74/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.1747 - acc: 0.7774 - val_loss: 0.2356 - val_acc: 0.6120\n",
      "Epoch 75/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.1721 - acc: 0.7826 - val_loss: 0.2515 - val_acc: 0.5946\n",
      "Epoch 76/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.1696 - acc: 0.7897 - val_loss: 0.2194 - val_acc: 0.6640\n",
      "Epoch 77/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.1669 - acc: 0.7944 - val_loss: 0.2122 - val_acc: 0.6924\n",
      "Epoch 78/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.1641 - acc: 0.7968 - val_loss: 0.2138 - val_acc: 0.6909\n",
      "Epoch 79/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.1609 - acc: 0.8058 - val_loss: 0.2136 - val_acc: 0.7019\n",
      "Epoch 80/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.1576 - acc: 0.8094 - val_loss: 0.1937 - val_acc: 0.7177\n",
      "Epoch 81/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.1544 - acc: 0.8169 - val_loss: 0.1993 - val_acc: 0.7145\n",
      "Epoch 82/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.1513 - acc: 0.8208 - val_loss: 0.2121 - val_acc: 0.6972\n",
      "Epoch 83/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.1488 - acc: 0.8264 - val_loss: 0.1930 - val_acc: 0.7129\n",
      "Epoch 84/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.1460 - acc: 0.8323 - val_loss: 0.1690 - val_acc: 0.7492\n",
      "Epoch 85/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.1442 - acc: 0.8307 - val_loss: 0.1792 - val_acc: 0.7366\n",
      "Epoch 86/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.1422 - acc: 0.8366 - val_loss: 0.1823 - val_acc: 0.7334\n",
      "Epoch 87/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.1401 - acc: 0.8354 - val_loss: 0.1766 - val_acc: 0.7413\n",
      "Epoch 88/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.1387 - acc: 0.8366 - val_loss: 0.1735 - val_acc: 0.7476\n",
      "Epoch 89/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.1369 - acc: 0.8386 - val_loss: 0.1615 - val_acc: 0.7650\n",
      "Epoch 90/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.1352 - acc: 0.8421 - val_loss: 0.1719 - val_acc: 0.7555\n",
      "Epoch 91/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.1341 - acc: 0.8429 - val_loss: 0.1636 - val_acc: 0.7697\n",
      "Epoch 92/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.1326 - acc: 0.8433 - val_loss: 0.1618 - val_acc: 0.7760\n",
      "Epoch 93/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.1308 - acc: 0.8461 - val_loss: 0.1489 - val_acc: 0.7918\n",
      "Epoch 94/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.1300 - acc: 0.8508 - val_loss: 0.1550 - val_acc: 0.7871\n",
      "Epoch 95/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.1282 - acc: 0.8481 - val_loss: 0.1412 - val_acc: 0.8028\n",
      "Epoch 96/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.1270 - acc: 0.8512 - val_loss: 0.1360 - val_acc: 0.8060\n",
      "Epoch 97/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.1258 - acc: 0.8548 - val_loss: 0.1516 - val_acc: 0.7965\n",
      "Epoch 98/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.1241 - acc: 0.8571 - val_loss: 0.1395 - val_acc: 0.8107\n",
      "Epoch 99/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.1229 - acc: 0.8591 - val_loss: 0.1408 - val_acc: 0.8091\n",
      "Epoch 100/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.1217 - acc: 0.8587 - val_loss: 0.1395 - val_acc: 0.8123\n",
      "Epoch 101/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.1203 - acc: 0.8611 - val_loss: 0.1388 - val_acc: 0.8139\n",
      "Epoch 102/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.1190 - acc: 0.8654 - val_loss: 0.1394 - val_acc: 0.8155\n",
      "Epoch 103/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.1177 - acc: 0.8666 - val_loss: 0.1405 - val_acc: 0.8170\n",
      "Epoch 104/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.1164 - acc: 0.8678 - val_loss: 0.1238 - val_acc: 0.8375\n",
      "Epoch 105/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.1155 - acc: 0.8682 - val_loss: 0.1343 - val_acc: 0.8249\n",
      "Epoch 106/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.1140 - acc: 0.8702 - val_loss: 0.1449 - val_acc: 0.8107\n",
      "Epoch 107/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.1135 - acc: 0.8694 - val_loss: 0.1380 - val_acc: 0.8218\n",
      "Epoch 108/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.1120 - acc: 0.8706 - val_loss: 0.1280 - val_acc: 0.8375\n",
      "Epoch 109/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.1108 - acc: 0.8737 - val_loss: 0.1266 - val_acc: 0.8407\n",
      "Epoch 110/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.1095 - acc: 0.8745 - val_loss: 0.1195 - val_acc: 0.8454\n",
      "Epoch 111/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.1086 - acc: 0.8781 - val_loss: 0.1233 - val_acc: 0.8423\n",
      "Epoch 112/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.1075 - acc: 0.8765 - val_loss: 0.1210 - val_acc: 0.8438\n",
      "Epoch 113/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.1064 - acc: 0.8796 - val_loss: 0.1184 - val_acc: 0.8438\n",
      "Epoch 114/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.1055 - acc: 0.8792 - val_loss: 0.1233 - val_acc: 0.8423\n",
      "Epoch 115/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.1045 - acc: 0.8812 - val_loss: 0.1097 - val_acc: 0.8580\n",
      "Epoch 116/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.1036 - acc: 0.8824 - val_loss: 0.1140 - val_acc: 0.8517\n",
      "Epoch 117/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.1026 - acc: 0.8844 - val_loss: 0.1186 - val_acc: 0.8486\n",
      "Epoch 118/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.1018 - acc: 0.8844 - val_loss: 0.1150 - val_acc: 0.8517\n",
      "Epoch 119/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.1007 - acc: 0.8863 - val_loss: 0.1038 - val_acc: 0.8628\n",
      "Epoch 120/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0999 - acc: 0.8875 - val_loss: 0.1083 - val_acc: 0.8628\n",
      "Epoch 121/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2534/2534 [==============================] - 0s - loss: 0.0990 - acc: 0.8891 - val_loss: 0.1102 - val_acc: 0.8596\n",
      "Epoch 122/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0981 - acc: 0.8891 - val_loss: 0.1056 - val_acc: 0.8644\n",
      "Epoch 123/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0973 - acc: 0.8907 - val_loss: 0.1106 - val_acc: 0.8596\n",
      "Epoch 124/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0966 - acc: 0.8934 - val_loss: 0.0954 - val_acc: 0.8801\n",
      "Epoch 125/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0959 - acc: 0.8907 - val_loss: 0.1040 - val_acc: 0.8675\n",
      "Epoch 126/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0949 - acc: 0.8931 - val_loss: 0.1074 - val_acc: 0.8675\n",
      "Epoch 127/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0943 - acc: 0.8931 - val_loss: 0.1101 - val_acc: 0.8628\n",
      "Epoch 128/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0935 - acc: 0.8942 - val_loss: 0.1167 - val_acc: 0.8565\n",
      "Epoch 129/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0930 - acc: 0.8958 - val_loss: 0.0944 - val_acc: 0.8849\n",
      "Epoch 130/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0924 - acc: 0.8966 - val_loss: 0.1003 - val_acc: 0.8770\n",
      "Epoch 131/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0914 - acc: 0.8990 - val_loss: 0.1020 - val_acc: 0.8754\n",
      "Epoch 132/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0907 - acc: 0.8978 - val_loss: 0.0993 - val_acc: 0.8801\n",
      "Epoch 133/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0901 - acc: 0.8990 - val_loss: 0.0943 - val_acc: 0.8833\n",
      "Epoch 134/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0896 - acc: 0.8986 - val_loss: 0.0921 - val_acc: 0.8833\n",
      "Epoch 135/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0890 - acc: 0.9006 - val_loss: 0.1018 - val_acc: 0.8754\n",
      "Epoch 136/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0883 - acc: 0.8994 - val_loss: 0.1065 - val_acc: 0.8738\n",
      "Epoch 137/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0877 - acc: 0.9009 - val_loss: 0.0857 - val_acc: 0.8959\n",
      "Epoch 138/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0872 - acc: 0.9006 - val_loss: 0.0904 - val_acc: 0.8896\n",
      "Epoch 139/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0865 - acc: 0.9021 - val_loss: 0.0889 - val_acc: 0.8927\n",
      "Epoch 140/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0860 - acc: 0.9013 - val_loss: 0.0927 - val_acc: 0.8896\n",
      "Epoch 141/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0857 - acc: 0.9033 - val_loss: 0.0886 - val_acc: 0.8943\n",
      "Epoch 142/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0848 - acc: 0.9025 - val_loss: 0.0944 - val_acc: 0.8849\n",
      "Epoch 143/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0843 - acc: 0.9045 - val_loss: 0.0895 - val_acc: 0.8943\n",
      "Epoch 144/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0838 - acc: 0.9057 - val_loss: 0.0889 - val_acc: 0.8943\n",
      "Epoch 145/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0832 - acc: 0.9061 - val_loss: 0.0825 - val_acc: 0.8991\n",
      "Epoch 146/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0827 - acc: 0.9049 - val_loss: 0.0816 - val_acc: 0.9006\n",
      "Epoch 147/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0822 - acc: 0.9065 - val_loss: 0.0837 - val_acc: 0.9006\n",
      "Epoch 148/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0815 - acc: 0.9065 - val_loss: 0.0837 - val_acc: 0.9006\n",
      "Epoch 149/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0811 - acc: 0.9077 - val_loss: 0.0971 - val_acc: 0.8817\n",
      "Epoch 150/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0808 - acc: 0.9073 - val_loss: 0.0887 - val_acc: 0.8943\n",
      "Epoch 151/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0799 - acc: 0.9084 - val_loss: 0.0845 - val_acc: 0.8975\n",
      "Epoch 152/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0795 - acc: 0.9088 - val_loss: 0.0754 - val_acc: 0.9101\n",
      "Epoch 153/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0789 - acc: 0.9112 - val_loss: 0.0844 - val_acc: 0.8959\n",
      "Epoch 154/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0783 - acc: 0.9108 - val_loss: 0.0870 - val_acc: 0.8943\n",
      "Epoch 155/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0778 - acc: 0.9108 - val_loss: 0.0755 - val_acc: 0.9101\n",
      "Epoch 156/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0774 - acc: 0.9096 - val_loss: 0.0797 - val_acc: 0.9069\n",
      "Epoch 157/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0767 - acc: 0.9124 - val_loss: 0.0832 - val_acc: 0.9006\n",
      "Epoch 158/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0761 - acc: 0.9120 - val_loss: 0.0775 - val_acc: 0.9069\n",
      "Epoch 159/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0753 - acc: 0.9132 - val_loss: 0.0929 - val_acc: 0.8864\n",
      "Epoch 160/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0751 - acc: 0.9124 - val_loss: 0.0821 - val_acc: 0.9022\n",
      "Epoch 161/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0742 - acc: 0.9128 - val_loss: 0.0793 - val_acc: 0.9038\n",
      "Epoch 162/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0736 - acc: 0.9155 - val_loss: 0.0740 - val_acc: 0.9101\n",
      "Epoch 163/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0732 - acc: 0.9163 - val_loss: 0.0711 - val_acc: 0.9117\n",
      "Epoch 164/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0723 - acc: 0.9155 - val_loss: 0.0824 - val_acc: 0.9022\n",
      "Epoch 165/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0714 - acc: 0.9171 - val_loss: 0.0796 - val_acc: 0.9022\n",
      "Epoch 166/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0706 - acc: 0.9171 - val_loss: 0.0758 - val_acc: 0.9054\n",
      "Epoch 167/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0697 - acc: 0.9175 - val_loss: 0.0779 - val_acc: 0.9038\n",
      "Epoch 168/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0690 - acc: 0.9175 - val_loss: 0.0666 - val_acc: 0.9148\n",
      "Epoch 169/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0681 - acc: 0.9183 - val_loss: 0.0705 - val_acc: 0.9101\n",
      "Epoch 170/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0670 - acc: 0.9203 - val_loss: 0.0841 - val_acc: 0.8927\n",
      "Epoch 171/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0659 - acc: 0.9203 - val_loss: 0.0769 - val_acc: 0.9054\n",
      "Epoch 172/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0651 - acc: 0.9195 - val_loss: 0.0661 - val_acc: 0.9164\n",
      "Epoch 173/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0639 - acc: 0.9223 - val_loss: 0.0655 - val_acc: 0.9180\n",
      "Epoch 174/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0626 - acc: 0.9242 - val_loss: 0.0855 - val_acc: 0.8864\n",
      "Epoch 175/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0618 - acc: 0.9258 - val_loss: 0.0640 - val_acc: 0.9196\n",
      "Epoch 176/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0615 - acc: 0.9266 - val_loss: 0.0894 - val_acc: 0.8817\n",
      "Epoch 177/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0595 - acc: 0.9278 - val_loss: 0.0601 - val_acc: 0.9196\n",
      "Epoch 178/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0589 - acc: 0.9286 - val_loss: 0.0618 - val_acc: 0.9180\n",
      "Epoch 179/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0575 - acc: 0.9298 - val_loss: 0.0765 - val_acc: 0.8975\n",
      "Epoch 180/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0569 - acc: 0.9317 - val_loss: 0.0674 - val_acc: 0.9117\n",
      "Epoch 181/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0562 - acc: 0.9325 - val_loss: 0.0582 - val_acc: 0.9227\n",
      "Epoch 182/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0549 - acc: 0.9337 - val_loss: 0.0672 - val_acc: 0.9085\n",
      "Epoch 183/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0544 - acc: 0.9337 - val_loss: 0.0721 - val_acc: 0.9022\n",
      "Epoch 184/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0537 - acc: 0.9345 - val_loss: 0.0595 - val_acc: 0.9196\n",
      "Epoch 185/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0525 - acc: 0.9373 - val_loss: 0.0605 - val_acc: 0.9196\n",
      "Epoch 186/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0522 - acc: 0.9337 - val_loss: 0.0642 - val_acc: 0.9132\n",
      "Epoch 187/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0515 - acc: 0.9369 - val_loss: 0.0727 - val_acc: 0.9006\n",
      "Epoch 188/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0510 - acc: 0.9361 - val_loss: 0.0602 - val_acc: 0.9196\n",
      "Epoch 189/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0501 - acc: 0.9400 - val_loss: 0.0598 - val_acc: 0.9211\n",
      "Epoch 190/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0496 - acc: 0.9380 - val_loss: 0.0568 - val_acc: 0.9290\n",
      "Epoch 191/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0490 - acc: 0.9408 - val_loss: 0.0535 - val_acc: 0.9338\n",
      "Epoch 192/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0486 - acc: 0.9380 - val_loss: 0.0637 - val_acc: 0.9148\n",
      "Epoch 193/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0482 - acc: 0.9404 - val_loss: 0.0764 - val_acc: 0.8959\n",
      "Epoch 194/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0475 - acc: 0.9424 - val_loss: 0.0684 - val_acc: 0.9054\n",
      "Epoch 195/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0475 - acc: 0.9412 - val_loss: 0.0645 - val_acc: 0.9085\n",
      "Epoch 196/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0471 - acc: 0.9408 - val_loss: 0.0567 - val_acc: 0.9259\n",
      "Epoch 197/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0465 - acc: 0.9432 - val_loss: 0.0618 - val_acc: 0.9148\n",
      "Epoch 198/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0461 - acc: 0.9416 - val_loss: 0.0577 - val_acc: 0.9274\n",
      "Epoch 199/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0459 - acc: 0.9424 - val_loss: 0.0616 - val_acc: 0.9164\n",
      "Epoch 200/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0455 - acc: 0.9424 - val_loss: 0.0666 - val_acc: 0.9101\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x126cadd30>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "# 1\n",
    "random.seed(datetime.now())\n",
    "\n",
    "# 2\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "model = Sequential()\n",
    "model.add(Dense(11, input_shape=(11,), init='uniform', activation='sigmoid'))\n",
    "model.add(Dense(11,init='uniform', activation='sigmoid'))\n",
    "model.add(Dense(11, init='uniform', activation='sigmoid'))\n",
    "model.add(Dense(1, init='uniform', activation='sigmoid'))\n",
    "model.summary()\n",
    "\n",
    "# 3\n",
    "import keras\n",
    "tbCallBack = keras.callbacks.TensorBoard(log_dir='/tmp/keras_logs', write_graph=True)\n",
    "\n",
    "# 4\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(dataframeX.values, dataframeY.values, epochs=200, batch_size=50,  verbose=1, validation_split=0.2, callbacks=[tbCallBack])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy of ~92% reached."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Case 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keeping all the columns, and increasing the number of neurons to maximum and epoch value to 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   meanfreq        sd    median       Q25       Q75       IQR       skew  \\\n",
      "0  0.059781  0.064241  0.032027  0.015071  0.090193  0.075122  12.863462   \n",
      "1  0.066009  0.067310  0.040229  0.019414  0.092666  0.073252  22.423285   \n",
      "2  0.077316  0.083829  0.036718  0.008701  0.131908  0.123207  30.757155   \n",
      "3  0.151228  0.072111  0.158011  0.096582  0.207955  0.111374   1.232831   \n",
      "4  0.135120  0.079146  0.124656  0.078720  0.206045  0.127325   1.101174   \n",
      "\n",
      "          kurt    sp.ent       sfm      mode  centroid   meanfun    minfun  \\\n",
      "0   274.402906  0.893369  0.491918  0.000000  0.059781  0.084279  0.015702   \n",
      "1   634.613855  0.892193  0.513724  0.000000  0.066009  0.107937  0.015826   \n",
      "2  1024.927705  0.846389  0.478905  0.000000  0.077316  0.098706  0.015656   \n",
      "3     4.177296  0.963322  0.727232  0.083878  0.151228  0.088965  0.017798   \n",
      "4     4.333713  0.971955  0.783568  0.104261  0.135120  0.106398  0.016931   \n",
      "\n",
      "     maxfun   meandom    mindom    maxdom   dfrange   modindx  \n",
      "0  0.275862  0.007812  0.007812  0.007812  0.000000  0.000000  \n",
      "1  0.250000  0.009014  0.007812  0.054688  0.046875  0.052632  \n",
      "2  0.271186  0.007990  0.007812  0.015625  0.007812  0.046512  \n",
      "3  0.250000  0.201497  0.007812  0.562500  0.554688  0.247119  \n",
      "4  0.266667  0.712812  0.007812  5.484375  5.476562  0.208274  \n"
     ]
    }
   ],
   "source": [
    "dataframeX = pds.read_csv('/Users/neelambabel/ADS Folder/Assignment 2/voice-data.csv', usecols=[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19])\n",
    "print(dataframeX.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  label\n",
      "0  male\n",
      "1  male\n",
      "2  male\n",
      "3  male\n",
      "4  male\n"
     ]
    }
   ],
   "source": [
    "dataframeY = pds.read_csv('/Users/neelambabel/ADS Folder/Assignment 2/voice-data.csv', usecols=[20])\n",
    "print(dataframeY.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   label\n",
      "0      0\n",
      "1      0\n",
      "2      0\n",
      "3      0\n",
      "4      0\n",
      "      label\n",
      "3163      1\n",
      "3164      1\n",
      "3165      1\n",
      "3166      1\n",
      "3167      1\n"
     ]
    }
   ],
   "source": [
    "def labelToInt(label):\n",
    "    if label == 'male':\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "    \n",
    "dataframeY.label = dataframeY.label.apply(labelToInt)\n",
    "print(dataframeY.head())\n",
    "print(dataframeY.tail())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/neelambabel/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:11: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(20, input_shape=(20,), activation=\"sigmoid\", kernel_initializer=\"uniform\")`\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/Users/neelambabel/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:12: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(20, activation=\"sigmoid\", kernel_initializer=\"uniform\")`\n",
      "  if sys.path[0] == '':\n",
      "/Users/neelambabel/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:13: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(20, activation=\"sigmoid\", kernel_initializer=\"uniform\")`\n",
      "  del sys.path[0]\n",
      "/Users/neelambabel/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:14: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1, activation=\"sigmoid\", kernel_initializer=\"uniform\")`\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_45 (Dense)             (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_46 (Dense)             (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_47 (Dense)             (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_48 (Dense)             (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 1,281\n",
      "Trainable params: 1,281\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 2534 samples, validate on 634 samples\n",
      "Epoch 1/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.2392 - acc: 0.6196 - val_loss: 0.3735 - val_acc: 0.0000e+00\n",
      "Epoch 2/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.2343 - acc: 0.6251 - val_loss: 0.3934 - val_acc: 0.0000e+00\n",
      "Epoch 3/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.2343 - acc: 0.6251 - val_loss: 0.3909 - val_acc: 0.0000e+00\n",
      "Epoch 4/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.2343 - acc: 0.6251 - val_loss: 0.3866 - val_acc: 0.0000e+00\n",
      "Epoch 5/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.2341 - acc: 0.6251 - val_loss: 0.3928 - val_acc: 0.0000e+00\n",
      "Epoch 6/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.2340 - acc: 0.6251 - val_loss: 0.3956 - val_acc: 0.0000e+00\n",
      "Epoch 7/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.2339 - acc: 0.6251 - val_loss: 0.3899 - val_acc: 0.0000e+00\n",
      "Epoch 8/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.2336 - acc: 0.6251 - val_loss: 0.3904 - val_acc: 0.0000e+00\n",
      "Epoch 9/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.2336 - acc: 0.6251 - val_loss: 0.3919 - val_acc: 0.0000e+00\n",
      "Epoch 10/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.2331 - acc: 0.6251 - val_loss: 0.3917 - val_acc: 0.0000e+00\n",
      "Epoch 11/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.2328 - acc: 0.6251 - val_loss: 0.4098 - val_acc: 0.0000e+00\n",
      "Epoch 12/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.2322 - acc: 0.6251 - val_loss: 0.3938 - val_acc: 0.0000e+00\n",
      "Epoch 13/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.2314 - acc: 0.6251 - val_loss: 0.3963 - val_acc: 0.0000e+00\n",
      "Epoch 14/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.2305 - acc: 0.6251 - val_loss: 0.3869 - val_acc: 0.0000e+00\n",
      "Epoch 15/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.2295 - acc: 0.6251 - val_loss: 0.3835 - val_acc: 0.0000e+00\n",
      "Epoch 16/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.2279 - acc: 0.6251 - val_loss: 0.3730 - val_acc: 0.0000e+00\n",
      "Epoch 17/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.2264 - acc: 0.6251 - val_loss: 0.3882 - val_acc: 0.0000e+00\n",
      "Epoch 18/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.2238 - acc: 0.6251 - val_loss: 0.3854 - val_acc: 0.0000e+00\n",
      "Epoch 19/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.2210 - acc: 0.6251 - val_loss: 0.3545 - val_acc: 0.0000e+00\n",
      "Epoch 20/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.2175 - acc: 0.6251 - val_loss: 0.3460 - val_acc: 0.0000e+00\n",
      "Epoch 21/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.2138 - acc: 0.6255 - val_loss: 0.3416 - val_acc: 0.0410\n",
      "Epoch 22/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.2090 - acc: 0.6602 - val_loss: 0.3375 - val_acc: 0.2003\n",
      "Epoch 23/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.2039 - acc: 0.7064 - val_loss: 0.3147 - val_acc: 0.3580\n",
      "Epoch 24/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.1989 - acc: 0.7407 - val_loss: 0.2795 - val_acc: 0.4937\n",
      "Epoch 25/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.1932 - acc: 0.7537 - val_loss: 0.2802 - val_acc: 0.4984\n",
      "Epoch 26/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.1880 - acc: 0.7723 - val_loss: 0.2817 - val_acc: 0.4858\n",
      "Epoch 27/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.1832 - acc: 0.7770 - val_loss: 0.2798 - val_acc: 0.5363\n",
      "Epoch 28/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.1784 - acc: 0.7837 - val_loss: 0.2868 - val_acc: 0.5110\n",
      "Epoch 29/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.1737 - acc: 0.7904 - val_loss: 0.2704 - val_acc: 0.5962\n",
      "Epoch 30/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.1695 - acc: 0.7948 - val_loss: 0.2611 - val_acc: 0.6215\n",
      "Epoch 31/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.1660 - acc: 0.7995 - val_loss: 0.2246 - val_acc: 0.6861\n",
      "Epoch 32/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.1627 - acc: 0.8027 - val_loss: 0.2552 - val_acc: 0.6278\n",
      "Epoch 33/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.1594 - acc: 0.8011 - val_loss: 0.2351 - val_acc: 0.6625\n",
      "Epoch 34/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.1558 - acc: 0.8094 - val_loss: 0.2505 - val_acc: 0.6309\n",
      "Epoch 35/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.1532 - acc: 0.8129 - val_loss: 0.2059 - val_acc: 0.7082\n",
      "Epoch 36/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.1499 - acc: 0.8197 - val_loss: 0.2407 - val_acc: 0.6593\n",
      "Epoch 37/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.1481 - acc: 0.8220 - val_loss: 0.1901 - val_acc: 0.7303\n",
      "Epoch 38/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.1459 - acc: 0.8197 - val_loss: 0.2172 - val_acc: 0.6972\n",
      "Epoch 39/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.1433 - acc: 0.8220 - val_loss: 0.1887 - val_acc: 0.7256\n",
      "Epoch 40/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.1413 - acc: 0.8275 - val_loss: 0.2225 - val_acc: 0.6972\n",
      "Epoch 41/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.1390 - acc: 0.8323 - val_loss: 0.1824 - val_acc: 0.7382\n",
      "Epoch 42/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.1372 - acc: 0.8358 - val_loss: 0.2120 - val_acc: 0.7082\n",
      "Epoch 43/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.1358 - acc: 0.8374 - val_loss: 0.1927 - val_acc: 0.7334\n",
      "Epoch 44/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.1343 - acc: 0.8382 - val_loss: 0.2123 - val_acc: 0.7161\n",
      "Epoch 45/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.1331 - acc: 0.8362 - val_loss: 0.1648 - val_acc: 0.7634\n",
      "Epoch 46/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.1313 - acc: 0.8437 - val_loss: 0.2070 - val_acc: 0.7177\n",
      "Epoch 47/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.1300 - acc: 0.8449 - val_loss: 0.1815 - val_acc: 0.7445\n",
      "Epoch 48/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.1287 - acc: 0.8429 - val_loss: 0.1736 - val_acc: 0.7539\n",
      "Epoch 49/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.1272 - acc: 0.8481 - val_loss: 0.1735 - val_acc: 0.7539\n",
      "Epoch 50/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.1260 - acc: 0.8489 - val_loss: 0.1911 - val_acc: 0.7334\n",
      "Epoch 51/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.1247 - acc: 0.8508 - val_loss: 0.1872 - val_acc: 0.7382\n",
      "Epoch 52/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.1233 - acc: 0.8493 - val_loss: 0.1748 - val_acc: 0.7555\n",
      "Epoch 53/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.1222 - acc: 0.8524 - val_loss: 0.1652 - val_acc: 0.7634\n",
      "Epoch 54/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.1214 - acc: 0.8508 - val_loss: 0.1869 - val_acc: 0.7413\n",
      "Epoch 55/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.1199 - acc: 0.8532 - val_loss: 0.1505 - val_acc: 0.7792\n",
      "Epoch 56/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.1190 - acc: 0.8575 - val_loss: 0.1679 - val_acc: 0.7634\n",
      "Epoch 57/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.1182 - acc: 0.8579 - val_loss: 0.1843 - val_acc: 0.7461\n",
      "Epoch 58/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.1167 - acc: 0.8611 - val_loss: 0.1738 - val_acc: 0.7571\n",
      "Epoch 59/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.1156 - acc: 0.8595 - val_loss: 0.1661 - val_acc: 0.7603\n",
      "Epoch 60/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.1147 - acc: 0.8639 - val_loss: 0.1699 - val_acc: 0.7603\n",
      "Epoch 61/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.1136 - acc: 0.8658 - val_loss: 0.1537 - val_acc: 0.7776\n",
      "Epoch 62/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.1126 - acc: 0.8674 - val_loss: 0.1706 - val_acc: 0.7618\n",
      "Epoch 63/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.1118 - acc: 0.8662 - val_loss: 0.1761 - val_acc: 0.7571\n",
      "Epoch 64/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.1108 - acc: 0.8662 - val_loss: 0.1597 - val_acc: 0.7713\n",
      "Epoch 65/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.1101 - acc: 0.8690 - val_loss: 0.1574 - val_acc: 0.7729\n",
      "Epoch 66/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.1090 - acc: 0.8745 - val_loss: 0.1378 - val_acc: 0.7997\n",
      "Epoch 67/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.1082 - acc: 0.8721 - val_loss: 0.1578 - val_acc: 0.7776\n",
      "Epoch 68/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.1075 - acc: 0.8749 - val_loss: 0.1703 - val_acc: 0.7603\n",
      "Epoch 69/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.1061 - acc: 0.8741 - val_loss: 0.1597 - val_acc: 0.7713\n",
      "Epoch 70/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.1056 - acc: 0.8765 - val_loss: 0.1584 - val_acc: 0.7792\n",
      "Epoch 71/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.1046 - acc: 0.8808 - val_loss: 0.1502 - val_acc: 0.7934\n",
      "Epoch 72/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.1033 - acc: 0.8812 - val_loss: 0.1372 - val_acc: 0.8013\n",
      "Epoch 73/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.1029 - acc: 0.8836 - val_loss: 0.1471 - val_acc: 0.7934\n",
      "Epoch 74/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.1016 - acc: 0.8863 - val_loss: 0.1615 - val_acc: 0.7823\n",
      "Epoch 75/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.1013 - acc: 0.8848 - val_loss: 0.1417 - val_acc: 0.7965\n",
      "Epoch 76/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.1005 - acc: 0.8852 - val_loss: 0.1394 - val_acc: 0.8013\n",
      "Epoch 77/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0997 - acc: 0.8879 - val_loss: 0.1592 - val_acc: 0.7808\n",
      "Epoch 78/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0987 - acc: 0.8887 - val_loss: 0.1535 - val_acc: 0.7886\n",
      "Epoch 79/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0988 - acc: 0.8879 - val_loss: 0.1495 - val_acc: 0.7886\n",
      "Epoch 80/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0970 - acc: 0.8907 - val_loss: 0.1475 - val_acc: 0.7934\n",
      "Epoch 81/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0960 - acc: 0.8931 - val_loss: 0.1554 - val_acc: 0.7871\n",
      "Epoch 82/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0956 - acc: 0.8942 - val_loss: 0.1402 - val_acc: 0.8013\n",
      "Epoch 83/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0948 - acc: 0.8954 - val_loss: 0.1426 - val_acc: 0.7965\n",
      "Epoch 84/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0945 - acc: 0.8950 - val_loss: 0.1484 - val_acc: 0.7918\n",
      "Epoch 85/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0940 - acc: 0.8958 - val_loss: 0.1271 - val_acc: 0.8218\n",
      "Epoch 86/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0939 - acc: 0.8954 - val_loss: 0.1483 - val_acc: 0.7902\n",
      "Epoch 87/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0929 - acc: 0.8962 - val_loss: 0.1455 - val_acc: 0.7950\n",
      "Epoch 88/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0919 - acc: 0.8962 - val_loss: 0.1348 - val_acc: 0.8123\n",
      "Epoch 89/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0915 - acc: 0.9006 - val_loss: 0.1628 - val_acc: 0.7792\n",
      "Epoch 90/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0909 - acc: 0.9002 - val_loss: 0.1264 - val_acc: 0.8297\n",
      "Epoch 91/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0898 - acc: 0.9017 - val_loss: 0.1521 - val_acc: 0.7902\n",
      "Epoch 92/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0894 - acc: 0.9009 - val_loss: 0.1408 - val_acc: 0.8060\n",
      "Epoch 93/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0889 - acc: 0.9037 - val_loss: 0.1210 - val_acc: 0.8391\n",
      "Epoch 94/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0894 - acc: 0.8986 - val_loss: 0.1154 - val_acc: 0.8438\n",
      "Epoch 95/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0882 - acc: 0.9021 - val_loss: 0.1268 - val_acc: 0.8297\n",
      "Epoch 96/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0883 - acc: 0.9029 - val_loss: 0.1319 - val_acc: 0.8186\n",
      "Epoch 97/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0869 - acc: 0.9073 - val_loss: 0.1357 - val_acc: 0.8155\n",
      "Epoch 98/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0865 - acc: 0.9061 - val_loss: 0.1125 - val_acc: 0.8502\n",
      "Epoch 99/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0857 - acc: 0.9077 - val_loss: 0.1225 - val_acc: 0.8375\n",
      "Epoch 100/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0854 - acc: 0.9088 - val_loss: 0.1327 - val_acc: 0.8281\n",
      "Epoch 101/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0848 - acc: 0.9077 - val_loss: 0.1291 - val_acc: 0.8297\n",
      "Epoch 102/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0844 - acc: 0.9092 - val_loss: 0.1147 - val_acc: 0.8470\n",
      "Epoch 103/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0843 - acc: 0.9100 - val_loss: 0.1281 - val_acc: 0.8297\n",
      "Epoch 104/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0837 - acc: 0.9108 - val_loss: 0.1405 - val_acc: 0.8186\n",
      "Epoch 105/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0832 - acc: 0.9120 - val_loss: 0.1166 - val_acc: 0.8423\n",
      "Epoch 106/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0831 - acc: 0.9108 - val_loss: 0.1069 - val_acc: 0.8580\n",
      "Epoch 107/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0829 - acc: 0.9104 - val_loss: 0.1307 - val_acc: 0.8265\n",
      "Epoch 108/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0824 - acc: 0.9100 - val_loss: 0.1301 - val_acc: 0.8297\n",
      "Epoch 109/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0819 - acc: 0.9104 - val_loss: 0.1181 - val_acc: 0.8407\n",
      "Epoch 110/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0819 - acc: 0.9104 - val_loss: 0.1076 - val_acc: 0.8549\n",
      "Epoch 111/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0824 - acc: 0.9104 - val_loss: 0.1256 - val_acc: 0.8281\n",
      "Epoch 112/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0812 - acc: 0.9116 - val_loss: 0.1204 - val_acc: 0.8375\n",
      "Epoch 113/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0811 - acc: 0.9124 - val_loss: 0.1241 - val_acc: 0.8328\n",
      "Epoch 114/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0819 - acc: 0.9104 - val_loss: 0.1189 - val_acc: 0.8407\n",
      "Epoch 115/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0806 - acc: 0.9120 - val_loss: 0.0985 - val_acc: 0.8675\n",
      "Epoch 116/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0808 - acc: 0.9120 - val_loss: 0.1268 - val_acc: 0.8328\n",
      "Epoch 117/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0794 - acc: 0.9140 - val_loss: 0.1040 - val_acc: 0.8596\n",
      "Epoch 118/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0795 - acc: 0.9144 - val_loss: 0.1054 - val_acc: 0.8580\n",
      "Epoch 119/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0794 - acc: 0.9132 - val_loss: 0.1177 - val_acc: 0.8454\n",
      "Epoch 120/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0794 - acc: 0.9128 - val_loss: 0.1180 - val_acc: 0.8407\n",
      "Epoch 121/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2534/2534 [==============================] - 0s - loss: 0.0783 - acc: 0.9155 - val_loss: 0.0978 - val_acc: 0.8754\n",
      "Epoch 122/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0786 - acc: 0.9152 - val_loss: 0.1092 - val_acc: 0.8549\n",
      "Epoch 123/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0782 - acc: 0.9152 - val_loss: 0.1159 - val_acc: 0.8486\n",
      "Epoch 124/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0787 - acc: 0.9140 - val_loss: 0.1085 - val_acc: 0.8533\n",
      "Epoch 125/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0778 - acc: 0.9144 - val_loss: 0.1141 - val_acc: 0.8533\n",
      "Epoch 126/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0776 - acc: 0.9155 - val_loss: 0.1220 - val_acc: 0.8360\n",
      "Epoch 127/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0778 - acc: 0.9152 - val_loss: 0.1065 - val_acc: 0.8580\n",
      "Epoch 128/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0768 - acc: 0.9155 - val_loss: 0.1171 - val_acc: 0.8438\n",
      "Epoch 129/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0769 - acc: 0.9136 - val_loss: 0.1234 - val_acc: 0.8438\n",
      "Epoch 130/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0773 - acc: 0.9159 - val_loss: 0.1371 - val_acc: 0.8249\n",
      "Epoch 131/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0766 - acc: 0.9152 - val_loss: 0.1083 - val_acc: 0.8580\n",
      "Epoch 132/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0761 - acc: 0.9163 - val_loss: 0.0981 - val_acc: 0.8707\n",
      "Epoch 133/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0770 - acc: 0.9136 - val_loss: 0.1174 - val_acc: 0.8438\n",
      "Epoch 134/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0764 - acc: 0.9159 - val_loss: 0.1112 - val_acc: 0.8580\n",
      "Epoch 135/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0760 - acc: 0.9171 - val_loss: 0.1042 - val_acc: 0.8644\n",
      "Epoch 136/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0762 - acc: 0.9155 - val_loss: 0.0995 - val_acc: 0.8707\n",
      "Epoch 137/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0753 - acc: 0.9167 - val_loss: 0.1087 - val_acc: 0.85960.9\n",
      "Epoch 138/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0754 - acc: 0.9175 - val_loss: 0.0879 - val_acc: 0.8864\n",
      "Epoch 139/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0749 - acc: 0.9179 - val_loss: 0.1127 - val_acc: 0.8549\n",
      "Epoch 140/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0747 - acc: 0.9163 - val_loss: 0.1031 - val_acc: 0.8675\n",
      "Epoch 141/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0748 - acc: 0.9163 - val_loss: 0.1068 - val_acc: 0.8612\n",
      "Epoch 142/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0746 - acc: 0.9187 - val_loss: 0.1238 - val_acc: 0.8391\n",
      "Epoch 143/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0743 - acc: 0.9175 - val_loss: 0.1270 - val_acc: 0.8360\n",
      "Epoch 144/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0744 - acc: 0.9163 - val_loss: 0.1174 - val_acc: 0.8454\n",
      "Epoch 145/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0758 - acc: 0.9144 - val_loss: 0.1444 - val_acc: 0.8186\n",
      "Epoch 146/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0742 - acc: 0.9179 - val_loss: 0.0940 - val_acc: 0.8785\n",
      "Epoch 147/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0736 - acc: 0.9191 - val_loss: 0.0963 - val_acc: 0.8722\n",
      "Epoch 148/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0740 - acc: 0.9171 - val_loss: 0.1022 - val_acc: 0.8659\n",
      "Epoch 149/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0730 - acc: 0.9187 - val_loss: 0.1151 - val_acc: 0.8517\n",
      "Epoch 150/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0729 - acc: 0.9195 - val_loss: 0.1187 - val_acc: 0.8470\n",
      "Epoch 151/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0727 - acc: 0.9195 - val_loss: 0.1105 - val_acc: 0.8565\n",
      "Epoch 152/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0731 - acc: 0.9199 - val_loss: 0.1081 - val_acc: 0.8580\n",
      "Epoch 153/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0724 - acc: 0.9207 - val_loss: 0.1034 - val_acc: 0.8659\n",
      "Epoch 154/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0721 - acc: 0.9211 - val_loss: 0.1092 - val_acc: 0.8565\n",
      "Epoch 155/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0716 - acc: 0.9211 - val_loss: 0.0938 - val_acc: 0.8785\n",
      "Epoch 156/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0724 - acc: 0.9207 - val_loss: 0.1050 - val_acc: 0.8580\n",
      "Epoch 157/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0712 - acc: 0.9234 - val_loss: 0.1283 - val_acc: 0.8375\n",
      "Epoch 158/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0712 - acc: 0.9207 - val_loss: 0.0945 - val_acc: 0.8785\n",
      "Epoch 159/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0716 - acc: 0.9203 - val_loss: 0.1194 - val_acc: 0.8438\n",
      "Epoch 160/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0711 - acc: 0.9187 - val_loss: 0.0884 - val_acc: 0.8864\n",
      "Epoch 161/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0716 - acc: 0.9187 - val_loss: 0.1042 - val_acc: 0.8644\n",
      "Epoch 162/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0704 - acc: 0.9211 - val_loss: 0.1070 - val_acc: 0.8596\n",
      "Epoch 163/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0700 - acc: 0.9203 - val_loss: 0.1028 - val_acc: 0.8644\n",
      "Epoch 164/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0699 - acc: 0.9191 - val_loss: 0.0867 - val_acc: 0.8833\n",
      "Epoch 165/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0692 - acc: 0.9219 - val_loss: 0.1047 - val_acc: 0.8628\n",
      "Epoch 166/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0689 - acc: 0.9219 - val_loss: 0.1061 - val_acc: 0.8580\n",
      "Epoch 167/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0683 - acc: 0.9234 - val_loss: 0.0694 - val_acc: 0.9085\n",
      "Epoch 168/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0682 - acc: 0.9227 - val_loss: 0.1127 - val_acc: 0.8517\n",
      "Epoch 169/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0679 - acc: 0.9219 - val_loss: 0.1001 - val_acc: 0.8675\n",
      "Epoch 170/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0683 - acc: 0.9223 - val_loss: 0.1107 - val_acc: 0.8533\n",
      "Epoch 171/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0671 - acc: 0.9227 - val_loss: 0.0934 - val_acc: 0.8722\n",
      "Epoch 172/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0669 - acc: 0.9223 - val_loss: 0.1144 - val_acc: 0.8502\n",
      "Epoch 173/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0671 - acc: 0.9230 - val_loss: 0.1140 - val_acc: 0.8486\n",
      "Epoch 174/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0660 - acc: 0.9242 - val_loss: 0.1010 - val_acc: 0.8644\n",
      "Epoch 175/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0662 - acc: 0.9234 - val_loss: 0.1047 - val_acc: 0.8565\n",
      "Epoch 176/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0652 - acc: 0.9215 - val_loss: 0.0931 - val_acc: 0.8754\n",
      "Epoch 177/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0645 - acc: 0.9234 - val_loss: 0.0908 - val_acc: 0.8754\n",
      "Epoch 178/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0652 - acc: 0.9219 - val_loss: 0.1092 - val_acc: 0.8533\n",
      "Epoch 179/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0635 - acc: 0.9254 - val_loss: 0.1225 - val_acc: 0.8360\n",
      "Epoch 180/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0634 - acc: 0.9254 - val_loss: 0.1211 - val_acc: 0.8407\n",
      "Epoch 181/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0619 - acc: 0.9262 - val_loss: 0.0863 - val_acc: 0.8817\n",
      "Epoch 182/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0600 - acc: 0.9305 - val_loss: 0.1308 - val_acc: 0.8233\n",
      "Epoch 183/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0601 - acc: 0.9290 - val_loss: 0.1026 - val_acc: 0.8628\n",
      "Epoch 184/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0594 - acc: 0.9270 - val_loss: 0.1081 - val_acc: 0.8549\n",
      "Epoch 185/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2534/2534 [==============================] - 0s - loss: 0.0588 - acc: 0.9250 - val_loss: 0.0943 - val_acc: 0.8738\n",
      "Epoch 186/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0581 - acc: 0.9286 - val_loss: 0.1052 - val_acc: 0.8596\n",
      "Epoch 187/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0569 - acc: 0.9298 - val_loss: 0.0739 - val_acc: 0.8927\n",
      "Epoch 188/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0564 - acc: 0.9282 - val_loss: 0.0823 - val_acc: 0.8833\n",
      "Epoch 189/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0563 - acc: 0.9301 - val_loss: 0.0939 - val_acc: 0.8722\n",
      "Epoch 190/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0571 - acc: 0.9298 - val_loss: 0.0710 - val_acc: 0.8959\n",
      "Epoch 191/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0559 - acc: 0.9278 - val_loss: 0.0897 - val_acc: 0.8738\n",
      "Epoch 192/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0554 - acc: 0.9305 - val_loss: 0.0664 - val_acc: 0.9038\n",
      "Epoch 193/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0548 - acc: 0.9270 - val_loss: 0.1076 - val_acc: 0.8517\n",
      "Epoch 194/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0530 - acc: 0.9345 - val_loss: 0.0982 - val_acc: 0.8644\n",
      "Epoch 195/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0533 - acc: 0.9341 - val_loss: 0.0655 - val_acc: 0.9038\n",
      "Epoch 196/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0545 - acc: 0.9325 - val_loss: 0.1081 - val_acc: 0.8502\n",
      "Epoch 197/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0527 - acc: 0.9349 - val_loss: 0.0851 - val_acc: 0.8864\n",
      "Epoch 198/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0524 - acc: 0.9341 - val_loss: 0.0555 - val_acc: 0.9227\n",
      "Epoch 199/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0525 - acc: 0.9357 - val_loss: 0.0696 - val_acc: 0.9006\n",
      "Epoch 200/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0517 - acc: 0.9357 - val_loss: 0.0649 - val_acc: 0.9085\n",
      "Epoch 201/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0512 - acc: 0.9349 - val_loss: 0.0920 - val_acc: 0.8722\n",
      "Epoch 202/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0519 - acc: 0.9349 - val_loss: 0.0628 - val_acc: 0.9132\n",
      "Epoch 203/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0501 - acc: 0.9400 - val_loss: 0.0697 - val_acc: 0.9085\n",
      "Epoch 204/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0494 - acc: 0.9388 - val_loss: 0.0999 - val_acc: 0.8628\n",
      "Epoch 205/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0496 - acc: 0.9388 - val_loss: 0.0793 - val_acc: 0.8896\n",
      "Epoch 206/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0489 - acc: 0.9376 - val_loss: 0.0674 - val_acc: 0.9085\n",
      "Epoch 207/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0482 - acc: 0.9404 - val_loss: 0.0880 - val_acc: 0.8817\n",
      "Epoch 208/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0486 - acc: 0.9392 - val_loss: 0.0772 - val_acc: 0.8991\n",
      "Epoch 209/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0472 - acc: 0.9424 - val_loss: 0.0989 - val_acc: 0.8675\n",
      "Epoch 210/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0467 - acc: 0.9424 - val_loss: 0.0817 - val_acc: 0.8943\n",
      "Epoch 211/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0468 - acc: 0.9412 - val_loss: 0.0757 - val_acc: 0.8975\n",
      "Epoch 212/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0464 - acc: 0.9432 - val_loss: 0.0696 - val_acc: 0.9069\n",
      "Epoch 213/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0469 - acc: 0.9400 - val_loss: 0.0488 - val_acc: 0.9306\n",
      "Epoch 214/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0457 - acc: 0.9432 - val_loss: 0.0797 - val_acc: 0.8943\n",
      "Epoch 215/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0447 - acc: 0.9428 - val_loss: 0.0821 - val_acc: 0.8912\n",
      "Epoch 216/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0447 - acc: 0.9428 - val_loss: 0.0911 - val_acc: 0.8785\n",
      "Epoch 217/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0450 - acc: 0.9440 - val_loss: 0.1135 - val_acc: 0.8533\n",
      "Epoch 218/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0447 - acc: 0.9463 - val_loss: 0.0850 - val_acc: 0.8896\n",
      "Epoch 219/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0435 - acc: 0.9451 - val_loss: 0.0880 - val_acc: 0.8849\n",
      "Epoch 220/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0432 - acc: 0.9463 - val_loss: 0.0951 - val_acc: 0.8785\n",
      "Epoch 221/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0431 - acc: 0.9483 - val_loss: 0.0833 - val_acc: 0.8896\n",
      "Epoch 222/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0432 - acc: 0.9483 - val_loss: 0.0613 - val_acc: 0.9101\n",
      "Epoch 223/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0426 - acc: 0.9451 - val_loss: 0.0560 - val_acc: 0.9132\n",
      "Epoch 224/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0424 - acc: 0.9451 - val_loss: 0.0725 - val_acc: 0.9038\n",
      "Epoch 225/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0420 - acc: 0.9451 - val_loss: 0.0559 - val_acc: 0.9148\n",
      "Epoch 226/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0418 - acc: 0.9455 - val_loss: 0.0492 - val_acc: 0.9243\n",
      "Epoch 227/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0419 - acc: 0.9475 - val_loss: 0.0748 - val_acc: 0.9006\n",
      "Epoch 228/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0415 - acc: 0.9479 - val_loss: 0.0754 - val_acc: 0.8975\n",
      "Epoch 229/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0409 - acc: 0.9495 - val_loss: 0.0711 - val_acc: 0.9022\n",
      "Epoch 230/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0409 - acc: 0.9495 - val_loss: 0.1080 - val_acc: 0.8596\n",
      "Epoch 231/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0420 - acc: 0.9487 - val_loss: 0.0638 - val_acc: 0.9054\n",
      "Epoch 232/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0395 - acc: 0.9538 - val_loss: 0.0729 - val_acc: 0.8975\n",
      "Epoch 233/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0396 - acc: 0.9511 - val_loss: 0.0757 - val_acc: 0.8991\n",
      "Epoch 234/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0394 - acc: 0.9522 - val_loss: 0.0749 - val_acc: 0.8912\n",
      "Epoch 235/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0394 - acc: 0.9479 - val_loss: 0.0575 - val_acc: 0.9148\n",
      "Epoch 236/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0389 - acc: 0.9503 - val_loss: 0.0679 - val_acc: 0.8991\n",
      "Epoch 237/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0398 - acc: 0.9479 - val_loss: 0.0669 - val_acc: 0.9038\n",
      "Epoch 238/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0408 - acc: 0.9487 - val_loss: 0.0628 - val_acc: 0.9069\n",
      "Epoch 239/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0393 - acc: 0.9503 - val_loss: 0.0678 - val_acc: 0.8959\n",
      "Epoch 240/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0391 - acc: 0.9519 - val_loss: 0.0464 - val_acc: 0.9353\n",
      "Epoch 241/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0388 - acc: 0.9515 - val_loss: 0.0725 - val_acc: 0.8991\n",
      "Epoch 242/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0373 - acc: 0.9530 - val_loss: 0.0626 - val_acc: 0.9085\n",
      "Epoch 243/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0373 - acc: 0.9534 - val_loss: 0.0757 - val_acc: 0.8912\n",
      "Epoch 244/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0383 - acc: 0.9515 - val_loss: 0.0693 - val_acc: 0.9006\n",
      "Epoch 245/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0373 - acc: 0.9550 - val_loss: 0.1060 - val_acc: 0.8644\n",
      "Epoch 246/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0378 - acc: 0.9522 - val_loss: 0.0781 - val_acc: 0.8880\n",
      "Epoch 247/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0370 - acc: 0.9550 - val_loss: 0.0822 - val_acc: 0.8864\n",
      "Epoch 248/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0361 - acc: 0.9562 - val_loss: 0.0924 - val_acc: 0.8707\n",
      "Epoch 249/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0367 - acc: 0.9538 - val_loss: 0.0659 - val_acc: 0.9054\n",
      "Epoch 250/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0366 - acc: 0.9526 - val_loss: 0.0891 - val_acc: 0.8754\n",
      "Epoch 251/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0362 - acc: 0.9570 - val_loss: 0.0891 - val_acc: 0.8785\n",
      "Epoch 252/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0356 - acc: 0.9562 - val_loss: 0.0583 - val_acc: 0.9148\n",
      "Epoch 253/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0354 - acc: 0.9546 - val_loss: 0.0513 - val_acc: 0.9211\n",
      "Epoch 254/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0353 - acc: 0.9562 - val_loss: 0.0529 - val_acc: 0.9180\n",
      "Epoch 255/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0360 - acc: 0.9550 - val_loss: 0.0479 - val_acc: 0.9259\n",
      "Epoch 256/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0361 - acc: 0.9542 - val_loss: 0.0683 - val_acc: 0.9006\n",
      "Epoch 257/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0353 - acc: 0.9574 - val_loss: 0.0787 - val_acc: 0.8959\n",
      "Epoch 258/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0347 - acc: 0.9554 - val_loss: 0.0597 - val_acc: 0.9132\n",
      "Epoch 259/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0358 - acc: 0.9534 - val_loss: 0.0897 - val_acc: 0.8817\n",
      "Epoch 260/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0343 - acc: 0.9578 - val_loss: 0.0561 - val_acc: 0.9132\n",
      "Epoch 261/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0337 - acc: 0.9586 - val_loss: 0.0799 - val_acc: 0.8959\n",
      "Epoch 262/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0335 - acc: 0.9586 - val_loss: 0.0696 - val_acc: 0.9038\n",
      "Epoch 263/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0333 - acc: 0.9597 - val_loss: 0.0635 - val_acc: 0.9054\n",
      "Epoch 264/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0329 - acc: 0.9594 - val_loss: 0.0579 - val_acc: 0.9085\n",
      "Epoch 265/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0326 - acc: 0.9601 - val_loss: 0.0713 - val_acc: 0.9006\n",
      "Epoch 266/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0324 - acc: 0.9601 - val_loss: 0.0789 - val_acc: 0.8959\n",
      "Epoch 267/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0319 - acc: 0.9601 - val_loss: 0.0816 - val_acc: 0.8912\n",
      "Epoch 268/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0325 - acc: 0.9597 - val_loss: 0.0606 - val_acc: 0.9085\n",
      "Epoch 269/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0328 - acc: 0.9597 - val_loss: 0.0647 - val_acc: 0.9038\n",
      "Epoch 270/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0325 - acc: 0.9605 - val_loss: 0.0493 - val_acc: 0.9196\n",
      "Epoch 271/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0325 - acc: 0.9594 - val_loss: 0.0443 - val_acc: 0.9338\n",
      "Epoch 272/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0318 - acc: 0.9625 - val_loss: 0.0674 - val_acc: 0.9022\n",
      "Epoch 273/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0319 - acc: 0.9605 - val_loss: 0.0338 - val_acc: 0.9511\n",
      "Epoch 274/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0309 - acc: 0.9641 - val_loss: 0.0579 - val_acc: 0.9117\n",
      "Epoch 275/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0316 - acc: 0.9605 - val_loss: 0.1076 - val_acc: 0.8675\n",
      "Epoch 276/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0316 - acc: 0.9613 - val_loss: 0.0572 - val_acc: 0.9132\n",
      "Epoch 277/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0309 - acc: 0.9629 - val_loss: 0.0836 - val_acc: 0.8880\n",
      "Epoch 278/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0316 - acc: 0.9629 - val_loss: 0.0668 - val_acc: 0.9054\n",
      "Epoch 279/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0305 - acc: 0.9629 - val_loss: 0.0517 - val_acc: 0.9180\n",
      "Epoch 280/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0313 - acc: 0.9609 - val_loss: 0.0592 - val_acc: 0.9085\n",
      "Epoch 281/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0308 - acc: 0.9645 - val_loss: 0.0404 - val_acc: 0.9401\n",
      "Epoch 282/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0300 - acc: 0.9645 - val_loss: 0.0435 - val_acc: 0.9338\n",
      "Epoch 283/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0306 - acc: 0.9633 - val_loss: 0.0521 - val_acc: 0.9180\n",
      "Epoch 284/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0302 - acc: 0.9617 - val_loss: 0.0578 - val_acc: 0.9148\n",
      "Epoch 285/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0306 - acc: 0.9621 - val_loss: 0.0781 - val_acc: 0.8975\n",
      "Epoch 286/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0307 - acc: 0.9621 - val_loss: 0.0721 - val_acc: 0.8975\n",
      "Epoch 287/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0298 - acc: 0.9629 - val_loss: 0.0856 - val_acc: 0.8864\n",
      "Epoch 288/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0299 - acc: 0.9645 - val_loss: 0.0474 - val_acc: 0.9290\n",
      "Epoch 289/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0313 - acc: 0.9617 - val_loss: 0.0580 - val_acc: 0.9132\n",
      "Epoch 290/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0292 - acc: 0.9653 - val_loss: 0.0506 - val_acc: 0.9227\n",
      "Epoch 291/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0303 - acc: 0.9625 - val_loss: 0.0665 - val_acc: 0.9054\n",
      "Epoch 292/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0295 - acc: 0.9649 - val_loss: 0.0527 - val_acc: 0.9196\n",
      "Epoch 293/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0288 - acc: 0.9637 - val_loss: 0.0654 - val_acc: 0.9054\n",
      "Epoch 294/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0301 - acc: 0.9633 - val_loss: 0.0367 - val_acc: 0.9495\n",
      "Epoch 295/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0287 - acc: 0.9665 - val_loss: 0.0545 - val_acc: 0.9180\n",
      "Epoch 296/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0292 - acc: 0.9621 - val_loss: 0.0710 - val_acc: 0.8991\n",
      "Epoch 297/300\n",
      "2534/2534 [==============================] - ETA: 0s - loss: 0.0292 - acc: 0.964 - 0s - loss: 0.0290 - acc: 0.9641 - val_loss: 0.0606 - val_acc: 0.9132\n",
      "Epoch 298/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0288 - acc: 0.9657 - val_loss: 0.0406 - val_acc: 0.9338\n",
      "Epoch 299/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0282 - acc: 0.9645 - val_loss: 0.0611 - val_acc: 0.9101\n",
      "Epoch 300/300\n",
      "2534/2534 [==============================] - 0s - loss: 0.0281 - acc: 0.9649 - val_loss: 0.0483 - val_acc: 0.9259\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x122eeef60>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "# 1\n",
    "random.seed(datetime.now())\n",
    "\n",
    "# 2\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "model = Sequential()\n",
    "model.add(Dense(20, input_shape=(20,), init='uniform', activation='sigmoid'))\n",
    "model.add(Dense(20, init='uniform', activation='sigmoid'))\n",
    "model.add(Dense(20,  init='uniform', activation='sigmoid'))\n",
    "model.add(Dense(1, init='uniform', activation='sigmoid'))\n",
    "model.summary()\n",
    "\n",
    "# 3\n",
    "import keras\n",
    "tbCallBack = keras.callbacks.TensorBoard(log_dir='/tmp/keras_logs', write_graph=True)\n",
    "\n",
    "# 4\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(dataframeX.values, dataframeY.values, epochs=300, batch_size=50,  verbose=1, validation_split=0.2, callbacks=[tbCallBack])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Accracy of ~94% reached."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Case 4\n",
    "\n",
    "Using a different activation function - ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   meanfreq        sd    median       Q25       Q75       IQR       skew  \\\n",
      "0  0.059781  0.064241  0.032027  0.015071  0.090193  0.075122  12.863462   \n",
      "1  0.066009  0.067310  0.040229  0.019414  0.092666  0.073252  22.423285   \n",
      "2  0.077316  0.083829  0.036718  0.008701  0.131908  0.123207  30.757155   \n",
      "3  0.151228  0.072111  0.158011  0.096582  0.207955  0.111374   1.232831   \n",
      "4  0.135120  0.079146  0.124656  0.078720  0.206045  0.127325   1.101174   \n",
      "\n",
      "          kurt    sp.ent       sfm      mode  centroid   meanfun    minfun  \\\n",
      "0   274.402906  0.893369  0.491918  0.000000  0.059781  0.084279  0.015702   \n",
      "1   634.613855  0.892193  0.513724  0.000000  0.066009  0.107937  0.015826   \n",
      "2  1024.927705  0.846389  0.478905  0.000000  0.077316  0.098706  0.015656   \n",
      "3     4.177296  0.963322  0.727232  0.083878  0.151228  0.088965  0.017798   \n",
      "4     4.333713  0.971955  0.783568  0.104261  0.135120  0.106398  0.016931   \n",
      "\n",
      "     maxfun   meandom    mindom    maxdom   dfrange   modindx  \n",
      "0  0.275862  0.007812  0.007812  0.007812  0.000000  0.000000  \n",
      "1  0.250000  0.009014  0.007812  0.054688  0.046875  0.052632  \n",
      "2  0.271186  0.007990  0.007812  0.015625  0.007812  0.046512  \n",
      "3  0.250000  0.201497  0.007812  0.562500  0.554688  0.247119  \n",
      "4  0.266667  0.712812  0.007812  5.484375  5.476562  0.208274  \n"
     ]
    }
   ],
   "source": [
    "dataframeX = pds.read_csv('/Users/neelambabel/ADS Folder/Assignment 2/voice-data.csv', usecols=[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19])\n",
    "print(dataframeX.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  label\n",
      "0  male\n",
      "1  male\n",
      "2  male\n",
      "3  male\n",
      "4  male\n"
     ]
    }
   ],
   "source": [
    "dataframeY = pds.read_csv('/Users/neelambabel/ADS Folder/Assignment 2/voice-data.csv', usecols=[20])\n",
    "print(dataframeY.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   label\n",
      "0      0\n",
      "1      0\n",
      "2      0\n",
      "3      0\n",
      "4      0\n",
      "      label\n",
      "3163      1\n",
      "3164      1\n",
      "3165      1\n",
      "3166      1\n",
      "3167      1\n"
     ]
    }
   ],
   "source": [
    "def labelToInt(label):\n",
    "    if label == 'male':\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "    \n",
    "dataframeY.label = dataframeY.label.apply(labelToInt)\n",
    "print(dataframeY.head())\n",
    "print(dataframeY.tail())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/neelambabel/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:11: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(20, input_shape=(20,), activation=\"ReLU\", kernel_initializer=\"uniform\")`\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unknown activation function:ReLU",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-22670efa42c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'uniform'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ReLU'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'uniform'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ReLU'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'uniform'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ReLU'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/neelambabel/anaconda/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/neelambabel/anaconda/lib/python3.6/site-packages/keras/layers/core.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, units, activation, use_bias, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, bias_constraint, **kwargs)\u001b[0m\n\u001b[1;32m    806\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    807\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 808\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    809\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_bias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muse_bias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel_initializer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitializers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel_initializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/neelambabel/anaconda/lib/python3.6/site-packages/keras/activations.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(identifier)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midentifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0midentifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midentifier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdeserialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midentifier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midentifier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midentifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/neelambabel/anaconda/lib/python3.6/site-packages/keras/activations.py\u001b[0m in \u001b[0;36mdeserialize\u001b[0;34m(name, custom_objects)\u001b[0m\n\u001b[1;32m     85\u001b[0m                                     \u001b[0mmodule_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m                                     \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m                                     printable_module_name='activation function')\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/neelambabel/anaconda/lib/python3.6/site-packages/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[1;32m    157\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m                 raise ValueError('Unknown ' + printable_module_name +\n\u001b[0;32m--> 159\u001b[0;31m                                  ':' + function_name)\n\u001b[0m\u001b[1;32m    160\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Unknown activation function:ReLU"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "# 1\n",
    "random.seed(datetime.now())\n",
    "\n",
    "# 2\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "model = Sequential()\n",
    "model.add(Dense(20, input_shape=(20,), init='uniform', activation='ReLU'))\n",
    "model.add(Dense(20,init='uniform', activation='ReLU'))\n",
    "model.add(Dense(20, init='uniform', activation='ReLU'))\n",
    "model.add(Dense(1, init='uniform', activation='ReLU'))\n",
    "model.summary()\n",
    "\n",
    "# 3\n",
    "import keras\n",
    "tbCallBack = keras.callbacks.TensorBoard(log_dir='/tmp/keras_logs', write_graph=True)\n",
    "\n",
    "# 4\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(dataframeX.values, dataframeY.values, epochs=500, batch_size=50,  verbose=1, validation_split=0.2, callbacks=[tbCallBack])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   meanfreq        sd    median       Q25       Q75       IQR       skew  \\\n",
      "0  0.059781  0.064241  0.032027  0.015071  0.090193  0.075122  12.863462   \n",
      "1  0.066009  0.067310  0.040229  0.019414  0.092666  0.073252  22.423285   \n",
      "2  0.077316  0.083829  0.036718  0.008701  0.131908  0.123207  30.757155   \n",
      "3  0.151228  0.072111  0.158011  0.096582  0.207955  0.111374   1.232831   \n",
      "4  0.135120  0.079146  0.124656  0.078720  0.206045  0.127325   1.101174   \n",
      "\n",
      "          kurt    sp.ent       sfm      mode  centroid   meanfun    minfun  \\\n",
      "0   274.402906  0.893369  0.491918  0.000000  0.059781  0.084279  0.015702   \n",
      "1   634.613855  0.892193  0.513724  0.000000  0.066009  0.107937  0.015826   \n",
      "2  1024.927705  0.846389  0.478905  0.000000  0.077316  0.098706  0.015656   \n",
      "3     4.177296  0.963322  0.727232  0.083878  0.151228  0.088965  0.017798   \n",
      "4     4.333713  0.971955  0.783568  0.104261  0.135120  0.106398  0.016931   \n",
      "\n",
      "     maxfun   meandom    mindom    maxdom   dfrange   modindx  \n",
      "0  0.275862  0.007812  0.007812  0.007812  0.000000  0.000000  \n",
      "1  0.250000  0.009014  0.007812  0.054688  0.046875  0.052632  \n",
      "2  0.271186  0.007990  0.007812  0.015625  0.007812  0.046512  \n",
      "3  0.250000  0.201497  0.007812  0.562500  0.554688  0.247119  \n",
      "4  0.266667  0.712812  0.007812  5.484375  5.476562  0.208274  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'iris = dataframeX()\\nprint(iris.data)\\n**print(iris.target)'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "import pandas as pds\n",
    "from sklearn import tree\n",
    "dataframeX = pds.read_csv(\"F://ADS//Assignment//voice-data.csv\", usecols=[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19])\n",
    "print(dataframeX.head())\n",
    "\n",
    "\n",
    "\"\"\"iris = dataframeX()\n",
    "print(iris.data)\n",
    "**print(iris.target)\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  label\n",
      "0  male\n",
      "1  male\n",
      "2  male\n",
      "3  male\n",
      "4  male\n"
     ]
    }
   ],
   "source": [
    "dataframeY = pds.read_csv(\"F://ADS//Assignment//voice-data.csv\", usecols=[20])\n",
    "print(dataframeY.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   label\n",
      "0      0\n",
      "1      0\n",
      "2      0\n",
      "3      0\n",
      "4      0\n",
      "      label\n",
      "3163      1\n",
      "3164      1\n",
      "3165      1\n",
      "3166      1\n",
      "3167      1\n"
     ]
    }
   ],
   "source": [
    "def labelToInt(label):\n",
    "    if label == 'male':\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "    \n",
    "dataframeY.label = dataframeY.label.apply(labelToInt)\n",
    "print(dataframeY.head())\n",
    "print(dataframeY.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(dataframeX, dataframeY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "def visualize_tree(tree,f):\n",
    "    \"\"\"Create tree png using graphviz.\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "    tree -- scikit-learn DecsisionTree.\n",
    "    \"\"\"  \n",
    "    with open(f, 'w') as f:\n",
    "        export_graphviz(tree, out_file=f) \n",
    "    command = [\"dot\", \"-Tpng\", f, \"-o\", \"o.png\"]\n",
    "    try:\n",
    "        subprocess.check_call(command)\n",
    "    except:\n",
    "        exit(\"Could not run dot, ie graphviz, to \"\n",
    "             \"produce visualization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_tree(clf, \"dataframeX.dot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
